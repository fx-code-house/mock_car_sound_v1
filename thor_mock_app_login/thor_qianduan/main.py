#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
alphaxç•Œé¢è®¾è®¡.sketch å®Œæ•´èµ„æºæå–å·¥å…·
ä¸“é—¨æå–ç¬¦å·ã€ç»„ä»¶ã€æ’ä»¶ã€ç”»æ¿ç­‰å®Œæ•´è®¾è®¡èµ„æº
"""

import zipfile
import json
import os
import shutil
import glob
from pathlib import Path
import xml.etree.ElementTree as ET
import base64
import re
from collections import defaultdict


def extract_sketch_completely():
    """å®Œæ•´æå–Sketchæ–‡ä»¶ä¸­çš„æ‰€æœ‰èµ„æº"""

    sketch_file = "alphaxç•Œé¢è®¾è®¡.sketch"

    if not os.path.exists(sketch_file):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{sketch_file}'")
        return False

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "alphax_å®Œæ•´è®¾è®¡èµ„æº"
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)

    os.makedirs(output_dir)

    try:
        print("ğŸ¨ å¼€å§‹å®Œæ•´æå– alphaxç•Œé¢è®¾è®¡.sketch...")
        print("=" * 60)

        # è§£å‹Sketchæ–‡ä»¶
        with zipfile.ZipFile(sketch_file, 'r') as zip_ref:
            zip_ref.extractall(output_dir)

        print("âœ… Sketchæ–‡ä»¶è§£å‹æˆåŠŸ")

        # åˆ›å»ºåˆ†ç±»ç›®å½•
        categories = {
            '00_æ‰€æœ‰å›¾ç‰‡': 'æ‰€æœ‰å›¾ç‰‡èµ„æº',
            '01_ç¬¦å·åº“_Symbols': 'ç¬¦å·å’Œç»„ä»¶',
            '02_ç”»æ¿_Artboards': 'æ‰€æœ‰ç”»æ¿',
            '03_é¡µé¢_Pages': 'é¡µé¢ç»“æ„',
            '04_æ ·å¼_Styles': 'å›¾å±‚æ ·å¼',
            '05_æ’ä»¶æ•°æ®': 'æ’ä»¶ç›¸å…³å†…å®¹',
            '06_åŸå§‹æ–‡ä»¶': 'åŸå§‹JSONæ•°æ®',
            '07_é¢„è§ˆå¯¼å‡º': 'å¯¼å‡ºé¢„è§ˆ',
        '08_UIå…ƒç´ è¯¦ç»†åˆ†æ': 'UIå…ƒç´ å’Œæ–‡å­—åˆ†æ'
        }

        for dir_name, dir_desc in categories.items():
            os.makedirs(os.path.join(output_dir, dir_name), exist_ok=True)

        # 1. æå–æ‰€æœ‰å›¾ç‰‡
        print("\nğŸ“¸ æå–å›¾ç‰‡èµ„æº...")
        extract_all_images(output_dir)

        # 2. æå–ç¬¦å·å’Œç»„ä»¶
        print("ğŸ”§ æå–ç¬¦å·å’Œç»„ä»¶...")
        extract_symbols_and_components(output_dir)

        # 3. æå–ç”»æ¿ä¿¡æ¯
        print("ğŸ¯ æå–ç”»æ¿ä¿¡æ¯...")
        extract_artboards(output_dir)

        # 4. æå–é¡µé¢ç»“æ„
        print("ğŸ“‘ æå–é¡µé¢ç»“æ„...")
        extract_pages_structure(output_dir)

        # 5. æå–æ’ä»¶æ•°æ®
        print("ğŸ”Œ æå–æ’ä»¶æ•°æ®...")
        extract_plugin_data(output_dir)

        # 6. æ·±åº¦æå–æ‰€æœ‰UIå…ƒç´ å’Œæ–‡å­—
        print("ğŸ” æ·±åº¦æ‰«æUIå…ƒç´ å’Œæ–‡å­—...")
        extract_all_ui_elements_and_texts(output_dir)

        # 7. å¯¼å‡ºå¯ç”¨çš„è®¾è®¡æ–‡ä»¶
        print("ğŸ¨ å¯¼å‡ºå¯ç”¨çš„è®¾è®¡èµ„æº...")
        export_usable_design_assets(output_dir)

        # 8. æŒ‰å±‚çº§å¯¼å‡ºPNGå…ƒç´ 
        print("ğŸ–¼ï¸ æŒ‰å±‚çº§å¯¼å‡ºPNGå…ƒç´ ...")
        export_hierarchical_png_elements(output_dir)

        # 9. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        print("ğŸ“Š ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
        generate_comprehensive_report(output_dir)

        print("\nğŸ‰ å®Œæ•´æå–å®Œæˆï¼")
        print("=" * 60)
        print_summary(output_dir)

        return True

    except Exception as e:
        print(f"âŒ æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def extract_all_images(output_dir):
    """æå–æ‰€æœ‰å›¾ç‰‡èµ„æºï¼ŒåŒ…æ‹¬åµŒå…¥å›¾ç‰‡å’Œbase64å›¾ç‰‡"""
    images_dir = os.path.join(output_dir, '00_æ‰€æœ‰å›¾ç‰‡')
    embedded_images_dir = os.path.join(images_dir, 'embedded')
    os.makedirs(embedded_images_dir, exist_ok=True)

    # 1. æå–æ–‡ä»¶ç³»ç»Ÿä¸­çš„å›¾ç‰‡
    image_extensions = ('*.png', '*.jpg', '*.jpeg', '*.gif', '*.webp', '*.svg', '*.bmp', '*.tiff')
    for extension in image_extensions:
        for image_path in glob.glob(os.path.join(output_dir, '**', extension), recursive=True):
            if '00_æ‰€æœ‰å›¾ç‰‡' not in image_path:
                filename = create_unique_filename(images_dir, os.path.basename(image_path))
                shutil.copy2(image_path, os.path.join(images_dir, filename))

    # 2. ä»JSONæ–‡ä»¶ä¸­æå–åµŒå…¥çš„base64å›¾ç‰‡å’Œå¼•ç”¨å›¾ç‰‡
    extract_embedded_images_from_json(output_dir, embedded_images_dir)

    # 3. æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡å¼•ç”¨
    extract_image_references(output_dir, images_dir)


def extract_symbols_and_components(output_dir):
    """æå–ç¬¦å·(Symbols)å’Œç»„ä»¶"""
    symbols_dir = os.path.join(output_dir, '01_ç¬¦å·åº“_Symbols')
    all_symbols = {}
    all_components = defaultdict(list)

    # æŸ¥æ‰¾document.jsonä¸­çš„ç¬¦å·
    document_path = os.path.join(output_dir, 'document.json')
    if os.path.exists(document_path):
        with open(document_path, 'r', encoding='utf-8') as f:
            document_data = json.load(f)

        # æå–ç¬¦å·ä¿¡æ¯ - ä¿®å¤é€»è¾‘
        if 'symbols' in document_data:
            symbols_data = document_data['symbols']
            if isinstance(symbols_data, dict) and 'objects' in symbols_data:
                symbols = symbols_data['objects']
            else:
                symbols = symbols_data
        else:
            symbols = []

        if symbols:
            symbols_info = {
                'symbols_count': len(symbols),
                'symbols_list': symbols
            }
            with open(os.path.join(symbols_dir, 'symbols_info.json'), 'w', encoding='utf-8') as f:
                json.dump(symbols_info, f, ensure_ascii=False, indent=2)
            all_symbols['document_symbols'] = symbols

        # æå–å›¾å±‚æ ·å¼
        layer_styles = document_data.get('layerStyles', {}).get('objects', [])
        if layer_styles:
            with open(os.path.join(output_dir, '04_æ ·å¼_Styles', 'layer_styles.json'), 'w', encoding='utf-8') as f:
                json.dump(layer_styles, f, ensure_ascii=False, indent=2)

        # æå–æ–‡æœ¬æ ·å¼
        text_styles = document_data.get('layerTextStyles', {}).get('objects', [])
        if text_styles:
            with open(os.path.join(output_dir, '04_æ ·å¼_Styles', 'text_styles.json'), 'w', encoding='utf-8') as f:
                json.dump(text_styles, f, ensure_ascii=False, indent=2)

    # æ·±åº¦è§£æé¡µé¢ä¸­çš„ç¬¦å·å’Œç»„ä»¶
    extract_symbols_from_pages(output_dir, symbols_dir, all_symbols, all_components)


def extract_artboards(output_dir):
    """æå–ç”»æ¿ä¿¡æ¯"""
    artboards_dir = os.path.join(output_dir, '02_ç”»æ¿_Artboards')
    all_artboards = {}
    artboard_count = 0

    # éå†pagesç›®å½•æŸ¥æ‰¾ç”»æ¿
    pages_dir = os.path.join(output_dir, 'pages')
    if os.path.exists(pages_dir):
        for page_file in os.listdir(pages_dir):
            if page_file.endswith('.json'):
                page_path = os.path.join(pages_dir, page_file)
                page_name = page_file.replace('.json', '')

                try:
                    with open(page_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # å¤„ç†å¤§æ–‡ä»¶
                    if len(content) > 1000000:  # 1MB
                        artboards = extract_artboards_from_large_file(content, page_name)
                    else:
                        page_data = json.loads(content)
                        artboards = extract_artboards_from_page_data(page_data, page_name)

                    if artboards:
                        page_artboards_dir = os.path.join(artboards_dir, page_name)
                        os.makedirs(page_artboards_dir, exist_ok=True)

                        for artboard_id, artboard_data in artboards.items():
                            artboard_file = os.path.join(page_artboards_dir, f"{artboard_id}.json")
                            with open(artboard_file, 'w', encoding='utf-8') as f:
                                json.dump(artboard_data, f, ensure_ascii=False, indent=2)

                            artboard_count += 1
                            print(f"  - æå–ç”»æ¿: {artboard_data.get('name', artboard_id)}")

                        all_artboards[page_name] = artboards

                except Exception as e:
                    print(f"  ! å¤„ç†é¡µé¢ {page_file} æ—¶å‡ºé”™: {e}")
                    continue

    # ä¿å­˜æ‰€æœ‰ç”»æ¿çš„æ‘˜è¦
    if all_artboards:
        summary_file = os.path.join(artboards_dir, 'artboards_summary.json')
        summary = {
            'total_artboards': artboard_count,
            'pages_with_artboards': list(all_artboards.keys()),
            'artboards_by_page': {
                page: [ab.get('name', aid) for aid, ab in artboards.items()]
                for page, artboards in all_artboards.items()
            }
        }
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

    return artboard_count


def extract_artboards_from_large_file(content, page_name):
    """ä»å¤§æ–‡ä»¶ä¸­æå–ç”»æ¿"""
    artboards = {}

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾artboard
    artboard_patterns = [
        r'"_class":\s*"MSArtboardGroup"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
        r'"artboard"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}'
    ]

    for pattern in artboard_patterns:
        matches = re.finditer(pattern, content, re.DOTALL)
        for i, match in enumerate(matches):
            try:
                artboard_text = match.group()
                # å°è¯•æ„å»ºå®Œæ•´çš„JSON
                if not artboard_text.startswith('{'):
                    artboard_text = '{' + artboard_text
                if not artboard_text.endswith('}'):
                    artboard_text = artboard_text + '}'

                artboard_data = json.loads(artboard_text)
                artboard_id = artboard_data.get('do_objectID', f'{page_name}_artboard_{i}')
                artboards[artboard_id] = artboard_data

            except json.JSONDecodeError:
                continue

    return artboards


def extract_artboards_from_page_data(page_data, page_name):
    """ä»é¡µé¢æ•°æ®ä¸­æå–ç”»æ¿"""
    artboards = {}

    def find_artboards_recursive(obj, path=""):
        if isinstance(obj, dict):
            if obj.get('_class') == 'MSArtboardGroup':
                artboard_id = obj.get('do_objectID', f'{page_name}_artboard_{len(artboards)}')
                artboards[artboard_id] = obj

            for key, value in obj.items():
                find_artboards_recursive(value, f"{path}.{key}")

        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                find_artboards_recursive(item, f"{path}[{i}]")

    # ç›´æ¥æŸ¥æ‰¾artboardså­—æ®µ
    if 'artboards' in page_data:
        direct_artboards = page_data['artboards']
        if isinstance(direct_artboards, dict):
            artboards.update(direct_artboards)

    # é€’å½’æŸ¥æ‰¾
    find_artboards_recursive(page_data)

    return artboards


def extract_pages_structure(output_dir):
    """æå–é¡µé¢ç»“æ„"""
    pages_dir = os.path.join(output_dir, '03_é¡µé¢_Pages')

    # å¤åˆ¶pagesç›®å½•
    source_pages = os.path.join(output_dir, 'pages')
    if os.path.exists(source_pages):
        shutil.copytree(source_pages, os.path.join(pages_dir, 'pages'))

    # æå–æ–‡æ¡£ç»“æ„
    document_path = os.path.join(output_dir, 'document.json')
    if os.path.exists(document_path):
        shutil.copy2(document_path, os.path.join(pages_dir, 'document_structure.json'))


def extract_plugin_data(output_dir):
    """æå–æ’ä»¶ç›¸å…³æ•°æ®å’Œå°å‹ç»„ä»¶"""
    plugin_dir = os.path.join(output_dir, '05_æ’ä»¶æ•°æ®')
    mini_components_dir = os.path.join(plugin_dir, 'mini_components')
    os.makedirs(mini_components_dir, exist_ok=True)

    plugin_summary = {
        'sketch_info': {},
        'user_data': {},
        'fonts': [],
        'mini_components': [],
        'external_references': [],
        'custom_data': {}
    }

    # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ’ä»¶æ•°æ®çš„æ–‡ä»¶
    plugin_indicators = ['plugin', 'metadata', 'user', 'workspace', 'preferences', 'settings']

    for root, dirs, files in os.walk(output_dir):
        for file in files:
            file_lower = file.lower()
            file_path = os.path.join(root, file)

            # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ’ä»¶æ•°æ®çš„æ–‡ä»¶
            if any(indicator in file_lower for indicator in plugin_indicators):
                relative_path = os.path.relpath(file_path, output_dir)
                dest_path = os.path.join(plugin_dir, relative_path)

                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                shutil.copy2(file_path, dest_path)
                print(f"  - å¤åˆ¶æ’ä»¶æ•°æ®: {relative_path}")

            # ç‰¹åˆ«å¤„ç†å„ç§é‡è¦æ–‡ä»¶
            if file == 'meta.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)

                plugin_summary['sketch_info'] = {
                    'appVersion': meta_data.get('appVersion'),
                    'build': meta_data.get('build'),
                    'version': meta_data.get('version'),
                    'variant': meta_data.get('variant'),
                    'created': meta_data.get('created'),
                    'commit': meta_data.get('commit')
                }

            elif file == 'user.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    user_data = json.load(f)
                plugin_summary['user_data'] = user_data
                print(f"  - æå–ç”¨æˆ·æ•°æ®: {len(user_data)} é¡¹é…ç½®")

            # å¤„ç†å­—ä½“æ–‡ä»¶
            elif file_lower.endswith(('.ttf', '.otf', '.woff', '.woff2')):
                font_info = {
                    'filename': file,
                    'path': relative_path,
                    'size': os.path.getsize(file_path)
                }
                plugin_summary['fonts'].append(font_info)

                # å¤åˆ¶å­—ä½“æ–‡ä»¶
                dest_path = os.path.join(plugin_dir, 'fonts', file)
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                shutil.copy2(file_path, dest_path)
                print(f"  - æå–å­—ä½“æ–‡ä»¶: {file}")

    # ä»JSONæ–‡ä»¶ä¸­æå–å°å‹ç»„ä»¶å’Œè‡ªå®šä¹‰æ•°æ®
    extract_mini_components_and_custom_data(output_dir, plugin_summary, mini_components_dir)

    # ä¿å­˜æ’ä»¶æ•°æ®æ‘˜è¦
    with open(os.path.join(plugin_dir, 'plugin_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(plugin_summary, f, ensure_ascii=False, indent=2)

    print(f"  - æ’ä»¶æ•°æ®æ€»ç»“: {len(plugin_summary['mini_components'])} ä¸ªå°å‹ç»„ä»¶")
    print(f"  - å­—ä½“æ–‡ä»¶: {len(plugin_summary['fonts'])} ä¸ª")


def extract_mini_components_and_custom_data(output_dir, plugin_summary, mini_components_dir):
    """æå–å°å‹ç»„ä»¶å’Œè‡ªå®šä¹‰æ•°æ®"""
    component_types = ['button', 'icon', 'badge', 'chip', 'tab', 'toggle', 'slider', 'input']

    for root, dirs, files in os.walk(output_dir):
        for file in files:
            if file.endswith('.json'):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # æŸ¥æ‰¾å°å‹ç»„ä»¶
                    for component_type in component_types:
                        pattern = rf'"{component_type}"[^{{}}]*{{[^{{}}]*(?:{{[^{{}}]*}}[^{{}}]*)*}}'
                        matches = re.finditer(pattern, content, re.IGNORECASE)

                        for i, match in enumerate(matches):
                            try:
                                component_text = match.group()
                                if not component_text.startswith('{'):
                                    component_text = '{' + component_text
                                if not component_text.endswith('}'):
                                    component_text = component_text + '}'

                                component_data = json.loads(component_text)
                                component_id = f"{component_type}_{file}_{i}"

                                component_info = {
                                    'id': component_id,
                                    'type': component_type,
                                    'source_file': file,
                                    'data': component_data
                                }

                                plugin_summary['mini_components'].append(component_info)

                                # ä¿å­˜å•ç‹¬çš„ç»„ä»¶æ–‡ä»¶
                                component_file = os.path.join(mini_components_dir, f"{component_id}.json")
                                with open(component_file, 'w', encoding='utf-8') as cf:
                                    json.dump(component_info, cf, ensure_ascii=False, indent=2)

                            except json.JSONDecodeError:
                                continue

                    # æŸ¥æ‰¾è‡ªå®šä¹‰æ•°æ®å’Œç”¨æˆ·å®šä¹‰
                    custom_patterns = [
                        r'"userInfo"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
                        r'"customData"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
                        r'"pluginData"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}'
                    ]

                    for pattern in custom_patterns:
                        matches = re.finditer(pattern, content, re.IGNORECASE)
                        for match in matches:
                            try:
                                custom_text = match.group()
                                if not custom_text.startswith('{'):
                                    custom_text = '{' + custom_text
                                if not custom_text.endswith('}'):
                                    custom_text = custom_text + '}'

                                custom_data = json.loads(custom_text)
                                key = f"{file}_{len(plugin_summary['custom_data'])}"
                                plugin_summary['custom_data'][key] = custom_data

                            except json.JSONDecodeError:
                                continue

                except Exception:
                    continue


def extract_all_ui_elements_and_texts(output_dir):
    """å¿«é€Ÿæå–æ‰€æœ‰UIå…ƒç´ ã€æ–‡å­—å’Œç»„ä»¶"""
    ui_elements_dir = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ')
    os.makedirs(ui_elements_dir, exist_ok=True)

    all_texts = []
    ui_element_counts = {
        'texts': 0,
        'buttons': 0,
        'groups': 0,
        'shapes': 0,
        'images': 0,
        'symbols': 0,
        'rectangles': 0,
        'paths': 0
    }

    # å¿«é€Ÿæ–‡å­—æå– - ä¸æ·±åº¦è§£æJSONï¼Œç›´æ¥ç”¨æ­£åˆ™
    pages_dir = os.path.join(output_dir, 'pages')
    if os.path.exists(pages_dir):
        for page_file in os.listdir(pages_dir):
            if page_file.endswith('.json'):
                page_path = os.path.join(pages_dir, page_file)
                page_name = page_file.replace('.json', '')

                print(f"  - å¿«é€Ÿæ‰«æé¡µé¢: {page_name}")

                try:
                    with open(page_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # å¿«é€Ÿæå–æ–‡å­— - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
                    extract_texts_fast(content, all_texts, page_name)

                    # å¿«é€Ÿç»Ÿè®¡UIå…ƒç´  - åªç»Ÿè®¡æ•°é‡ï¼Œä¸æ·±åº¦è§£æ
                    count_ui_elements_fast(content, ui_element_counts)

                except Exception as e:
                    print(f"    ! å¤„ç†é¡µé¢ {page_file} æ—¶å‡ºé”™: {e}")
                    continue

    # ä¿å­˜æå–ç»“æœ
    save_fast_ui_analysis(ui_elements_dir, all_texts, ui_element_counts)

    print(f"  âœ… å¿«é€Ÿæ‰«æå®Œæˆ: æ–‡å­—{len(all_texts)}ä¸ª, ç»„{ui_element_counts['groups']}ä¸ª, çŸ©å½¢{ui_element_counts['rectangles']}ä¸ª")


def extract_texts_fast(content, all_texts, page_name):
    """å¿«é€Ÿæå–æ–‡å­—å†…å®¹"""
    # ä¼˜åŒ–çš„æ–‡å­—æå–æ­£åˆ™è¡¨è¾¾å¼
    text_patterns = [
        r'"string":\s*"([^"]{2,})"',  # å­—ç¬¦ä¸²å­—æ®µï¼Œè‡³å°‘2ä¸ªå­—ç¬¦
        r'"name":\s*"([^"]*[\u4e00-\u9fa5]+[^"]*)"',  # åŒ…å«ä¸­æ–‡çš„åç§°
        r'"name":\s*"([^"]*(?:æŒ‰é’®|æ–‡æœ¬|æ ‡ç­¾|æç¤º|ç¡®å®š|å–æ¶ˆ|ç™»å½•|æ¢è½¦|æˆ‘çš„|è®¾ç½®|ä¸‹è½½)[^"]*)"',  # å¸¸è§UIæ–‡å­—
        r'(?<="string":\s")[^"]*[\u4e00-\u9fa5]+[^"]*(?=")',  # ç›´æ¥åŒ¹é…ä¸­æ–‡å­—ç¬¦ä¸²å†…å®¹
    ]

    for pattern in text_patterns:
        matches = re.finditer(pattern, content)
        for match in matches:
            text_content = match.group(1) if len(match.groups()) > 0 else match.group(0)
            if text_content and len(text_content.strip()) > 1:
                # å»é™¤å¸¸è§çš„æ— æ„ä¹‰æ–‡å­—
                if not any(skip in text_content.lower() for skip in ['uuid', 'object', 'class', 'frame', 'null']):
                    all_texts.append({
                        'text': text_content.strip(),
                        'source': page_name,
                        'type': 'fast_extract'
                    })


def count_ui_elements_fast(content, ui_counts):
    """å¿«é€Ÿç»Ÿè®¡UIå…ƒç´ æ•°é‡"""
    element_patterns = {
        'texts': r'"_class":\s*"text"',
        'groups': r'"_class":\s*"group"',
        'rectangles': r'"_class":\s*"rectangle"',
        'symbols': r'"_class":\s*"symbol',
        'shapes': r'"_class":\s*"shape',
        'buttons': r'(?:"name":[^}]*button|button[^}]*"name")',
        'paths': r'"_class":\s*".*[Pp]ath'
    }

    for element_type, pattern in element_patterns.items():
        matches = re.findall(pattern, content, re.IGNORECASE)
        ui_counts[element_type] += len(matches)


def save_fast_ui_analysis(ui_elements_dir, all_texts, ui_counts):
    """ä¿å­˜å¿«é€ŸUIåˆ†æç»“æœ"""

    # å»é‡å¹¶æ’åºæ–‡å­—
    unique_texts = []
    seen_texts = set()
    for text_item in all_texts:
        text = text_item['text']
        if text and text not in seen_texts and len(text.strip()) > 1:
            seen_texts.add(text)
            unique_texts.append(text_item)

    # æŒ‰é•¿åº¦æ’åºï¼ŒçŸ­æ–‡å­—ä¼˜å…ˆï¼ˆé€šå¸¸æ˜¯UIæ ‡ç­¾ï¼‰
    unique_texts.sort(key=lambda x: len(x['text']))

    # ä¿å­˜æ–‡å­—æ‘˜è¦
    with open(os.path.join(ui_elements_dir, 'texts_summary.txt'), 'w', encoding='utf-8') as f:
        f.write("=== æ‰€æœ‰æå–çš„æ–‡å­—å†…å®¹ ===\n\n")
        f.write(f"æ€»å…±æ‰¾åˆ° {len(unique_texts)} ä¸ªä¸é‡å¤çš„æ–‡å­—å…ƒç´ \n\n")

        # åˆ†ç±»æ˜¾ç¤º
        chinese_texts = [t for t in unique_texts if re.search(r'[\u4e00-\u9fa5]', t['text'])]
        english_texts = [t for t in unique_texts if not re.search(r'[\u4e00-\u9fa5]', t['text'])]

        f.write(f"=== ä¸­æ–‡æ–‡å­—å†…å®¹ ({len(chinese_texts)}ä¸ª) ===\n")
        for i, text_item in enumerate(chinese_texts, 1):
            f.write(f"{i:3d}. {text_item['text']} (æ¥æº: {text_item['source']})\n")

        f.write(f"\n=== è‹±æ–‡/æ•°å­—å†…å®¹ ({len(english_texts)}ä¸ª) ===\n")
        for i, text_item in enumerate(english_texts, 1):
            f.write(f"{i:3d}. {text_item['text']} (æ¥æº: {text_item['source']})\n")

    # ä¿å­˜å®Œæ•´æ•°æ®
    summary_data = {
        'extraction_summary': {
            'total_texts_found': len(all_texts),
            'unique_texts': len(unique_texts),
            'chinese_texts': len(chinese_texts),
            'english_texts': len(english_texts),
            'ui_elements_by_type': ui_counts,
            'total_ui_elements': sum(ui_counts.values())
        },
        'all_unique_texts': unique_texts,
        'ui_element_counts': ui_counts
    }

    with open(os.path.join(ui_elements_dir, 'ui_analysis_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)

    print(f"    - æ–‡å­—æ‘˜è¦æ–‡ä»¶: texts_summary.txt ({len(unique_texts)} ä¸ªä¸é‡å¤æ–‡å­—)")
    print(f"    - ä¸­æ–‡æ–‡å­—: {len(chinese_texts)} ä¸ª")
    print(f"    - è‹±æ–‡æ–‡å­—: {len(english_texts)} ä¸ª")


def export_usable_design_assets(output_dir):
    """å¯¼å‡ºå¯ç›´æ¥ä½¿ç”¨çš„è®¾è®¡èµ„æº"""
    design_assets_dir = os.path.join(output_dir, '09_å¯ç”¨è®¾è®¡èµ„æº')
    os.makedirs(design_assets_dir, exist_ok=True)

    # 1. åˆ›å»ºUIç»„ä»¶åº“æ–‡æ¡£
    create_ui_component_library(design_assets_dir, output_dir)

    # 2. å¯¼å‡ºæ–‡å­—æ ·å¼è¡¨
    export_text_styles(design_assets_dir, output_dir)

    # 3. ç”Ÿæˆé¢œè‰²é¢æ¿
    extract_color_palette(design_assets_dir, output_dir)

    # 4. åˆ›å»ºå±‚çº§ç»“æ„å±•ç¤º
    create_hierarchical_structure(design_assets_dir, output_dir)

    # 5. åˆ›å»ºè®¾è®¡è§„èŒƒæ–‡æ¡£
    create_design_specification(design_assets_dir, output_dir)

    print("  âœ… è®¾è®¡èµ„æºå¯¼å‡ºå®Œæˆ")


def create_ui_component_library(design_assets_dir, output_dir):
    """åˆ›å»ºUIç»„ä»¶åº“æ–‡æ¡£"""
    components_dir = os.path.join(design_assets_dir, 'ç»„ä»¶åº“')
    os.makedirs(components_dir, exist_ok=True)

    # è¯»å–UIåˆ†ææ•°æ®
    ui_summary_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'ui_analysis_summary.json')
    if not os.path.exists(ui_summary_file):
        return

    with open(ui_summary_file, 'r', encoding='utf-8') as f:
        ui_data = json.load(f)

    texts = ui_data.get('all_unique_texts', [])
    ui_counts = ui_data.get('ui_element_counts', {})

    # åˆ›å»ºæŒ‰é’®ç»„ä»¶æ–‡æ¡£
    create_button_components(components_dir, texts, ui_counts)

    # åˆ›å»ºæ–‡å­—ç»„ä»¶æ–‡æ¡£
    create_text_components(components_dir, texts)

    # åˆ›å»ºå¸ƒå±€ç»„ä»¶æ–‡æ¡£
    create_layout_components(components_dir, ui_counts)


def create_button_components(components_dir, texts, ui_counts):
    """åˆ›å»ºæŒ‰é’®ç»„ä»¶æ–‡æ¡£"""
    button_texts = [t['text'] for t in texts if any(keyword in t['text'] for keyword in
                   ['ç¡®å®š', 'å–æ¶ˆ', 'ç™»å½•', 'è®¾ç½®', 'ä¸‹è½½', 'æ¢è½¦', 'å®Œæˆ', 'å¥½çš„', 'æ”¯æŒ'])]

    button_doc = os.path.join(components_dir, 'æŒ‰é’®ç»„ä»¶.md')
    with open(button_doc, 'w', encoding='utf-8') as f:
        f.write("# æŒ‰é’®ç»„ä»¶åº“\n\n")
        f.write("## ä¸»è¦æŒ‰é’®æ–‡å­—\n\n")

        for i, text in enumerate(button_texts, 1):
            f.write(f"### {i}. {text}\n")
            f.write(f"```\n")
            f.write(f"æ–‡å­—: {text}\n")
            f.write(f"ç”¨é€”: ç”¨æˆ·æ“ä½œæŒ‰é’®\n")
            f.write(f"å»ºè®®æ ·å¼: ä¸»è¦æŒ‰é’® / æ¬¡è¦æŒ‰é’®\n")
            f.write(f"```\n\n")

        f.write("## è®¾è®¡å»ºè®®\n\n")
        f.write("- ä¸»è¦æ“ä½œæŒ‰é’®ï¼šç¡®å®šã€ç™»å½•ã€å®Œæˆ\n")
        f.write("- æ¬¡è¦æ“ä½œæŒ‰é’®ï¼šå–æ¶ˆã€è®¾ç½®ã€æ”¯æŒ\n")
        f.write("- åŠŸèƒ½æŒ‰é’®ï¼šä¸‹è½½ã€æ¢è½¦\n\n")

    print(f"    - æŒ‰é’®ç»„ä»¶: {len(button_texts)} ä¸ª")


def create_text_components(components_dir, texts):
    """åˆ›å»ºæ–‡å­—ç»„ä»¶æ–‡æ¡£"""
    # æŒ‰ç±»å‹åˆ†ç±»æ–‡å­—
    categories = {
        'å¯¼èˆªæ–‡å­—': ['é¦–é¡µ', 'æˆ‘çš„', 'è®¾ç½®'],
        'åŠŸèƒ½æ ‡ç­¾': ['æˆ‘çš„è½¦', 'æˆ‘çš„æ±½è»Š', 'è“ç‰™é“¾æ¥', 'è²æµªè¨­ç½®'],
        'çŠ¶æ€æ–‡å­—': ['å·²é€£æ¥', 'æ­£åœ¨ä¸‹è¼‰', 'offline', 'Scanning'],
        'è½¦å‹ä¿¡æ¯': ['mercedes-benz', 'amg', 'Camry', 'sedan'],
        'ç‰ˆæœ¬ä¿¡æ¯': ['1.0.0', 'w168', 'v168'],
        'æç¤ºæ–‡å­—': ['æç¤ºå¼¹æ¡†1', 'æç¤ºå¼¹æ¡†2', 'è¯·è¾“å…¥æ–°è®¾å¤‡']
    }

    text_doc = os.path.join(components_dir, 'æ–‡å­—ç»„ä»¶.md')
    with open(text_doc, 'w', encoding='utf-8') as f:
        f.write("# æ–‡å­—ç»„ä»¶åº“\n\n")

        for category, keywords in categories.items():
            f.write(f"## {category}\n\n")

            # æ‰¾åˆ°åŒ¹é…çš„æ–‡å­—
            matched_texts = []
            for text_item in texts:
                text = text_item['text']
                if any(keyword.lower() in text.lower() for keyword in keywords):
                    matched_texts.append(text)

            # å»é‡å¹¶æ’åº
            unique_texts = sorted(list(set(matched_texts)))

            for text in unique_texts[:10]:  # æ¯ç±»æœ€å¤šæ˜¾ç¤º10ä¸ª
                f.write(f"- {text}\n")

            if len(unique_texts) > 10:
                f.write(f"- ... è¿˜æœ‰ {len(unique_texts) - 10} ä¸ª\n")

            f.write("\n")

    print(f"    - æ–‡å­—ç»„ä»¶: {len(categories)} ä¸ªåˆ†ç±»")


def create_layout_components(components_dir, ui_counts):
    """åˆ›å»ºå¸ƒå±€ç»„ä»¶æ–‡æ¡£"""
    layout_doc = os.path.join(components_dir, 'å¸ƒå±€ç»„ä»¶.md')
    with open(layout_doc, 'w', encoding='utf-8') as f:
        f.write("# å¸ƒå±€ç»„ä»¶ç»Ÿè®¡\n\n")
        f.write("## UIå…ƒç´ æ•°é‡ç»Ÿè®¡\n\n")

        element_names = {
            'groups': 'ç¼–ç»„/å®¹å™¨',
            'rectangles': 'çŸ©å½¢/å¡ç‰‡',
            'texts': 'æ–‡å­—å…ƒç´ ',
            'shapes': 'å›¾å½¢/å›¾æ ‡',
            'symbols': 'ç¬¦å·/ç»„ä»¶',
            'paths': 'è·¯å¾„/çº¿æ¡'
        }

        for element_type, count in ui_counts.items():
            name = element_names.get(element_type, element_type)
            f.write(f"- **{name}**: {count} ä¸ª\n")

        f.write("\n## è®¾è®¡å»ºè®®\n\n")
        f.write("### å¸ƒå±€ç»“æ„\n")
        f.write(f"- ä¸»è¦ä½¿ç”¨ {ui_counts.get('groups', 0)} ä¸ªç¼–ç»„æ¥ç»„ç»‡å†…å®¹\n")
        f.write(f"- {ui_counts.get('rectangles', 0)} ä¸ªçŸ©å½¢å…ƒç´ ç”¨äºå¡ç‰‡å’ŒèƒŒæ™¯\n")
        f.write(f"- {ui_counts.get('shapes', 0)} ä¸ªå›¾å½¢å…ƒç´ ç”¨äºå›¾æ ‡å’Œè£…é¥°\n\n")

        f.write("### ç»„ä»¶å¤ç”¨\n")
        f.write(f"- å¯ä»¥å°† {ui_counts.get('symbols', 0)} ä¸ªç¬¦å·åˆ¶ä½œæˆå¯å¤ç”¨ç»„ä»¶\n")
        f.write("- å»ºè®®æ ‡å‡†åŒ–æŒ‰é’®ã€è¾“å…¥æ¡†ã€å¡ç‰‡ç­‰å¸¸ç”¨å…ƒç´ \n")


def export_text_styles(design_assets_dir, output_dir):
    """å¯¼å‡ºæ–‡å­—æ ·å¼è¡¨"""
    styles_dir = os.path.join(design_assets_dir, 'æ ·å¼è§„èŒƒ')
    os.makedirs(styles_dir, exist_ok=True)

    # è¯»å–æ–‡å­—æ•°æ®
    ui_summary_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'ui_analysis_summary.json')
    if not os.path.exists(ui_summary_file):
        return

    with open(ui_summary_file, 'r', encoding='utf-8') as f:
        ui_data = json.load(f)

    texts = ui_data.get('all_unique_texts', [])

    # æŒ‰é•¿åº¦åˆ†ç±»æ–‡å­—æ ·å¼
    text_styles = {
        'æ ‡é¢˜æ–‡å­—': [t['text'] for t in texts if len(t['text']) > 8 and any(word in t['text'] for word in ['è¨­ç½®', 'æ±½è»Š', 'è²æµª', 'æŠ€è¡“'])],
        'æŒ‰é’®æ–‡å­—': [t['text'] for t in texts if len(t['text']) <= 4 and t['text'] in ['ç¡®å®š', 'å–æ¶ˆ', 'è®¾ç½®', 'ç™»å½•', 'ä¸‹è½½', 'æ¢è½¦']],
        'æ ‡ç­¾æ–‡å­—': [t['text'] for t in texts if 3 <= len(t['text']) <= 8],
        'æè¿°æ–‡å­—': [t['text'] for t in texts if len(t['text']) > 15]
    }

    styles_doc = os.path.join(styles_dir, 'æ–‡å­—æ ·å¼è§„èŒƒ.md')
    with open(styles_doc, 'w', encoding='utf-8') as f:
        f.write("# æ–‡å­—æ ·å¼è§„èŒƒ\n\n")

        style_specs = {
            'æ ‡é¢˜æ–‡å­—': {'size': '18-24px', 'weight': 'Semibold', 'usage': 'é¡µé¢æ ‡é¢˜ã€åŠŸèƒ½æ¨¡å—åç§°'},
            'æŒ‰é’®æ–‡å­—': {'size': '14-16px', 'weight': 'Medium', 'usage': 'æŒ‰é’®æ ‡ç­¾ã€æ“ä½œæ–‡å­—'},
            'æ ‡ç­¾æ–‡å­—': {'size': '12-14px', 'weight': 'Regular', 'usage': 'å­—æ®µæ ‡ç­¾ã€çŠ¶æ€æç¤º'},
            'æè¿°æ–‡å­—': {'size': '12px', 'weight': 'Regular', 'usage': 'è¯¦ç»†è¯´æ˜ã€å¸®åŠ©æ–‡æœ¬'}
        }

        for style_type, texts in text_styles.items():
            if not texts:
                continue

            spec = style_specs.get(style_type, {})
            f.write(f"## {style_type}\n\n")
            f.write(f"**å»ºè®®è§„æ ¼:**\n")
            f.write(f"- å­—å·: {spec.get('size', '14px')}\n")
            f.write(f"- å­—é‡: {spec.get('weight', 'Regular')}\n")
            f.write(f"- ç”¨é€”: {spec.get('usage', 'é€šç”¨æ–‡å­—')}\n\n")

            f.write("**ç¤ºä¾‹æ–‡å­—:**\n")
            for text in texts[:5]:  # æ¯ç±»æ˜¾ç¤º5ä¸ªç¤ºä¾‹
                f.write(f"- {text}\n")

            if len(texts) > 5:
                f.write(f"- ... è¿˜æœ‰ {len(texts) - 5} ä¸ª\n")

            f.write("\n")

    print(f"    - æ–‡å­—æ ·å¼: {len(text_styles)} ä¸ªåˆ†ç±»")


def extract_color_palette(design_assets_dir, output_dir):
    """æå–é¢œè‰²é¢æ¿"""
    colors_dir = os.path.join(design_assets_dir, 'é¢œè‰²è§„èŒƒ')
    os.makedirs(colors_dir, exist_ok=True)

    # ä»æ ·å¼æ–‡ä»¶ä¸­æå–é¢œè‰²
    styles_dir = os.path.join(output_dir, '04_æ ·å¼_Styles')
    colors = []

    if os.path.exists(styles_dir):
        for file in os.listdir(styles_dir):
            if file.endswith('.json'):
                file_path = os.path.join(styles_dir, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # ç®€å•æå–é¢œè‰²å€¼
                        color_matches = re.findall(r'"red":\s*([\d.]+).*?"green":\s*([\d.]+).*?"blue":\s*([\d.]+)', content)
                        for r, g, b in color_matches:
                            colors.append({'r': float(r), 'g': float(g), 'b': float(b)})
                except:
                    continue

    # ç”Ÿæˆé¢œè‰²è§„èŒƒæ–‡æ¡£
    colors_doc = os.path.join(colors_dir, 'é¢œè‰²è§„èŒƒ.md')
    with open(colors_doc, 'w', encoding='utf-8') as f:
        f.write("# é¢œè‰²è§„èŒƒ\n\n")

        if colors:
            f.write("## æå–çš„é¢œè‰²å€¼\n\n")
            unique_colors = []
            for color in colors[:20]:  # å–å‰20ä¸ªé¢œè‰²
                rgb = f"rgb({int(color['r']*255)}, {int(color['g']*255)}, {int(color['b']*255)})"
                if rgb not in unique_colors:
                    unique_colors.append(rgb)

            for i, color in enumerate(unique_colors, 1):
                f.write(f"{i}. {color}\n")
        else:
            f.write("## å»ºè®®çš„é¢œè‰²è§„èŒƒ\n\n")
            f.write("åŸºäºæ±½è½¦åº”ç”¨çš„ç‰¹ç‚¹ï¼Œå»ºè®®ä½¿ç”¨ä»¥ä¸‹é¢œè‰²ï¼š\n\n")
            f.write("- **ä¸»è‰²è°ƒ**: #2C3E50 (æ·±è“ç°)\n")
            f.write("- **è¾…åŠ©è‰²**: #3498DB (è“è‰²)\n")
            f.write("- **æˆåŠŸè‰²**: #27AE60 (ç»¿è‰²)\n")
            f.write("- **è­¦å‘Šè‰²**: #F39C12 (æ©™è‰²)\n")
            f.write("- **é”™è¯¯è‰²**: #E74C3C (çº¢è‰²)\n")
            f.write("- **æ–‡å­—è‰²**: #2C3E50 (æ·±ç°)\n")
            f.write("- **èƒŒæ™¯è‰²**: #FFFFFF (ç™½è‰²)\n")

    print(f"    - é¢œè‰²è§„èŒƒ: {len(colors)} ä¸ªé¢œè‰²å€¼")


def create_design_specification(design_assets_dir, output_dir):
    """åˆ›å»ºè®¾è®¡è§„èŒƒæ–‡æ¡£"""
    spec_file = os.path.join(design_assets_dir, 'è®¾è®¡è§„èŒƒæ€»è§ˆ.md')

    with open(spec_file, 'w', encoding='utf-8') as f:
        f.write("# alphax æ±½è½¦åº”ç”¨è®¾è®¡è§„èŒƒ\n\n")
        f.write("åŸºäºSketchæ–‡ä»¶æå–çš„å®Œæ•´è®¾è®¡è§„èŒƒ\n\n")

        f.write("## ğŸ“± åº”ç”¨æ¦‚è¿°\n\n")
        f.write("è¿™æ˜¯ä¸€ä¸ªæ±½è½¦å£°éŸ³æ§åˆ¶åº”ç”¨ï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\n")
        f.write("- è½¦è¾†è¿æ¥å’Œç®¡ç†\n")
        f.write("- å£°éŸ³è®¾ç½®å’Œè°ƒèŠ‚\n")
        f.write("- ç”¨æˆ·è´¦æˆ·ç®¡ç†\n")
        f.write("- è®¾å¤‡è®¾ç½®å’Œæ”¯æŒ\n\n")

        f.write("## ğŸ¨ è®¾è®¡æ–‡ä»¶ç»“æ„\n\n")
        f.write("```\n")
        f.write("09_å¯ç”¨è®¾è®¡èµ„æº/\n")
        f.write("â”œâ”€â”€ ç»„ä»¶åº“/\n")
        f.write("â”‚   â”œâ”€â”€ æŒ‰é’®ç»„ä»¶.md\n")
        f.write("â”‚   â”œâ”€â”€ æ–‡å­—ç»„ä»¶.md\n")
        f.write("â”‚   â””â”€â”€ å¸ƒå±€ç»„ä»¶.md\n")
        f.write("â”œâ”€â”€ æ ·å¼è§„èŒƒ/\n")
        f.write("â”‚   â””â”€â”€ æ–‡å­—æ ·å¼è§„èŒƒ.md\n")
        f.write("â”œâ”€â”€ é¢œè‰²è§„èŒƒ/\n")
        f.write("â”‚   â””â”€â”€ é¢œè‰²è§„èŒƒ.md\n")
        f.write("â””â”€â”€ è®¾è®¡è§„èŒƒæ€»è§ˆ.md\n")
        f.write("```\n\n")

        f.write("## ğŸ”§ å¦‚ä½•ä½¿ç”¨è¿™äº›èµ„æº\n\n")
        f.write("### 1. å¼€å‘å›¢é˜Ÿ\n")
        f.write("- æŸ¥çœ‹ `ç»„ä»¶åº“/` äº†è§£æ‰€æœ‰UIç»„ä»¶\n")
        f.write("- å‚è€ƒ `æ ·å¼è§„èŒƒ/` å®ç°æ–‡å­—æ ·å¼\n")
        f.write("- ä½¿ç”¨ `é¢œè‰²è§„èŒƒ/` ä¿æŒé¢œè‰²ä¸€è‡´æ€§\n\n")

        f.write("### 2. è®¾è®¡å›¢é˜Ÿ\n")
        f.write("- åŸºäºç»„ä»¶åº“åˆ›å»ºè®¾è®¡ç³»ç»Ÿ\n")
        f.write("- æ ‡å‡†åŒ–æŒ‰é’®ã€æ–‡å­—ã€é¢œè‰²ç­‰å…ƒç´ \n")
        f.write("- ç¡®ä¿è®¾è®¡ä¸€è‡´æ€§\n\n")

        f.write("### 3. äº§å“å›¢é˜Ÿ\n")
        f.write("- äº†è§£ç°æœ‰åŠŸèƒ½å’ŒUIå…ƒç´ \n")
        f.write("- è§„åˆ’æ–°åŠŸèƒ½æ—¶å‚è€ƒç°æœ‰ç»„ä»¶\n")
        f.write("- ç»´æŠ¤äº§å“è®¾è®¡è§„èŒƒ\n\n")

    print("    - è®¾è®¡è§„èŒƒæ–‡æ¡£å·²ç”Ÿæˆ")


def create_hierarchical_structure(design_assets_dir, output_dir):
    """åˆ›å»ºå±‚çº§ç»“æ„å±•ç¤º"""
    hierarchy_dir = os.path.join(design_assets_dir, 'å±‚çº§ç»“æ„')
    os.makedirs(hierarchy_dir, exist_ok=True)

    # 1. åˆ›å»ºé¡µé¢å±‚çº§ç»“æ„
    create_page_hierarchy(hierarchy_dir, output_dir)

    # 2. åˆ›å»ºç»„ä»¶å±‚çº§ç»“æ„
    create_component_hierarchy(hierarchy_dir, output_dir)

    # 3. åˆ›å»ºåŠŸèƒ½æ¨¡å—å±‚çº§
    create_functional_hierarchy(hierarchy_dir, output_dir)

    print("    - å±‚çº§ç»“æ„å±•ç¤ºå·²ç”Ÿæˆ")


def create_page_hierarchy(hierarchy_dir, output_dir):
    """åˆ›å»ºé¡µé¢å±‚çº§ç»“æ„"""
    ui_summary_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'ui_analysis_summary.json')
    if not os.path.exists(ui_summary_file):
        return

    with open(ui_summary_file, 'r', encoding='utf-8') as f:
        ui_data = json.load(f)

    texts = ui_data.get('all_unique_texts', [])

    # æŒ‰é¡µé¢åˆ†ç»„æ–‡å­—å†…å®¹
    page_structure = {}
    for text_item in texts:
        source = text_item['source']
        text = text_item['text']

        if source not in page_structure:
            page_structure[source] = {
                'ç•Œé¢æ ‡é¢˜': [],
                'åŠŸèƒ½æŒ‰é’®': [],
                'å¯¼èˆªå…ƒç´ ': [],
                'çŠ¶æ€ä¿¡æ¯': [],
                'è¾“å…¥æ§ä»¶': [],
                'è½¦å‹ç›¸å…³': [],
                'å…¶ä»–å†…å®¹': []
            }

        # æ™ºèƒ½åˆ†ç±»æ–‡å­—åˆ°ä¸åŒå±‚çº§
        if any(keyword in text for keyword in ['é¡µé¢', 'ç•Œé¢', 'ä¸»é¡µ', 'é¦–é¡µ']):
            page_structure[source]['ç•Œé¢æ ‡é¢˜'].append(text)
        elif any(keyword in text for keyword in ['ç¡®å®š', 'å–æ¶ˆ', 'ç™»å½•', 'è®¾ç½®', 'ä¸‹è½½', 'æ¢è½¦', 'å®Œæˆ']):
            page_structure[source]['åŠŸèƒ½æŒ‰é’®'].append(text)
        elif any(keyword in text for keyword in ['æˆ‘çš„', 'é¦–é¡µ', 'è®¾ç½®', 'æ”¯æŒ']):
            page_structure[source]['å¯¼èˆªå…ƒç´ '].append(text)
        elif any(keyword in text for keyword in ['å·²è¿æ¥', 'æ­£åœ¨', 'offline', 'Scanning', 'è²éŸ³è³‡æº']):
            page_structure[source]['çŠ¶æ€ä¿¡æ¯'].append(text)
        elif any(keyword in text for keyword in ['è¾“å…¥', 'è¯·è¾“å…¥', 'è®¾å¤‡å·', 'å¼¹æ¡†']):
            page_structure[source]['è¾“å…¥æ§ä»¶'].append(text)
        elif any(keyword in text for keyword in ['mercedes', 'amg', 'Camry', 'sedan', 'w168', 'v168']):
            page_structure[source]['è½¦å‹ç›¸å…³'].append(text)
        else:
            page_structure[source]['å…¶ä»–å†…å®¹'].append(text)

    # ç”Ÿæˆå±‚çº§ç»“æ„æ–‡æ¡£
    hierarchy_file = os.path.join(hierarchy_dir, 'é¡µé¢å±‚çº§ç»“æ„.md')
    with open(hierarchy_file, 'w', encoding='utf-8') as f:
        f.write("# é¡µé¢å±‚çº§ç»“æ„\n\n")
        f.write("## ğŸ“± åº”ç”¨æ•´ä½“æ¶æ„\n\n")

        page_names = {
            '3A32758E-96E1-4F29-B73E-616E68E0C6DA': 'ä¸»ç•Œé¢é¡µé¢',
            'A37E5689-D4A7-426A-97B6-30682016EE6E': 'æ§åˆ¶é¢æ¿é¡µé¢'
        }

        for page_id, content in page_structure.items():
            page_name = page_names.get(page_id, f'é¡µé¢_{page_id[:8]}')
            f.write(f"### ğŸ¯ {page_name}\n\n")

            for category, items in content.items():
                if items:
                    f.write(f"#### ğŸ“‚ {category}\n")
                    for item in sorted(set(items))[:10]:  # å»é‡å¹¶å–å‰10ä¸ª
                        f.write(f"    â”œâ”€â”€ {item}\n")
                    if len(set(items)) > 10:
                        f.write(f"    â””â”€â”€ ... è¿˜æœ‰ {len(set(items)) - 10} ä¸ª\n")
                    f.write("\n")


def create_component_hierarchy(hierarchy_dir, output_dir):
    """åˆ›å»ºç»„ä»¶å±‚çº§ç»“æ„"""
    ui_summary_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'ui_analysis_summary.json')
    if not os.path.exists(ui_summary_file):
        return

    with open(ui_summary_file, 'r', encoding='utf-8') as f:
        ui_data = json.load(f)

    ui_counts = ui_data.get('ui_element_counts', {})
    texts = ui_data.get('all_unique_texts', [])

    # åˆ›å»ºç»„ä»¶å±‚çº§ç»“æ„
    hierarchy_file = os.path.join(hierarchy_dir, 'ç»„ä»¶å±‚çº§ç»“æ„.md')
    with open(hierarchy_file, 'w', encoding='utf-8') as f:
        f.write("# UIç»„ä»¶å±‚çº§ç»“æ„\n\n")
        f.write("## ğŸ—ï¸ ç»„ä»¶æ¶æ„æ€»è§ˆ\n\n")

        f.write("```\n")
        f.write("alphaxæ±½è½¦åº”ç”¨\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ ğŸ–¼ï¸ å¸ƒå±€å®¹å™¨å±‚\n")
        f.write(f"â”‚   â”œâ”€ ç¼–ç»„/å®¹å™¨: {ui_counts.get('groups', 0)} ä¸ª\n")
        f.write(f"â”‚   â””â”€ çŸ©å½¢/å¡ç‰‡: {ui_counts.get('rectangles', 0)} ä¸ª\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ ğŸ¨ è§†è§‰å…ƒç´ å±‚\n")
        f.write(f"â”‚   â”œâ”€ å›¾å½¢/å›¾æ ‡: {ui_counts.get('shapes', 0)} ä¸ª\n")
        f.write(f"â”‚   â”œâ”€ è·¯å¾„/çº¿æ¡: {ui_counts.get('paths', 0)} ä¸ª\n")
        f.write(f"â”‚   â””â”€ ç¬¦å·ç»„ä»¶: {ui_counts.get('symbols', 0)} ä¸ª\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ ğŸ“ æ–‡å­—å†…å®¹å±‚\n")
        f.write(f"â”‚   â””â”€ æ–‡å­—å…ƒç´ : {ui_counts.get('texts', 0)} ä¸ª\n")
        f.write("â”‚\n")
        f.write("â””â”€ ğŸ”§ äº¤äº’åŠŸèƒ½å±‚\n")
        f.write("    â”œâ”€ æŒ‰é’®ç»„ä»¶\n")
        f.write("    â”œâ”€ è¾“å…¥æ§ä»¶\n")
        f.write("    â””â”€ çŠ¶æ€æŒ‡ç¤º\n")
        f.write("```\n\n")

        # æŒ‰åŠŸèƒ½æ¨¡å—åˆ†ç±»ç»„ä»¶
        f.write("## ğŸ›ï¸ åŠŸèƒ½æ¨¡å—ç»„ä»¶\n\n")

        modules = {
            'è½¦è¾†ç®¡ç†æ¨¡å—': ['æˆ‘çš„è½¦', 'æ¢è½¦', 'è½¦å‹', 'mercedes', 'amg', 'Camry'],
            'éŸ³é¢‘æ§åˆ¶æ¨¡å—': ['è²æµª', 'éŸ³é‡', 'å£°éŸ³', 'ä¸‹è½½', 'æ€ é€Ÿ'],
            'è¿æ¥è®¾ç½®æ¨¡å—': ['è“ç‰™', 'è¿æ¥', 'è®¾å¤‡', 'è¾“å…¥æ–°è®¾å¤‡'],
            'ç”¨æˆ·ç•Œé¢æ¨¡å—': ['ç™»å½•', 'è®¾ç½®', 'é¦–é¡µ', 'æˆ‘çš„', 'æ”¯æŒ'],
            'çŠ¶æ€æ˜¾ç¤ºæ¨¡å—': ['å·²è¿æ¥', 'æ­£åœ¨ä¸‹è½½', 'offline', 'Scanning']
        }

        for module, keywords in modules.items():
            f.write(f"### ğŸ“¦ {module}\n")

            # æ‰¾åˆ°ç›¸å…³çš„æ–‡å­—
            related_texts = []
            for text_item in texts:
                text = text_item['text']
                if any(keyword.lower() in text.lower() for keyword in keywords):
                    related_texts.append(text)

            # å»é‡å¹¶åˆ†å±‚æ˜¾ç¤º
            unique_texts = sorted(list(set(related_texts)))

            if unique_texts:
                f.write("```\n")
                for i, text in enumerate(unique_texts[:8], 1):  # æ¯æ¨¡å—æ˜¾ç¤º8ä¸ª
                    f.write(f"â”œâ”€ {text}\n")
                if len(unique_texts) > 8:
                    f.write(f"â””â”€ ... è¿˜æœ‰ {len(unique_texts) - 8} ä¸ªç›¸å…³å…ƒç´ \n")
                f.write("```\n")
            else:
                f.write("æš‚æ— åŒ¹é…çš„ç»„ä»¶\n")
            f.write("\n")


def create_functional_hierarchy(hierarchy_dir, output_dir):
    """åˆ›å»ºåŠŸèƒ½æ¨¡å—å±‚çº§"""
    ui_summary_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'ui_analysis_summary.json')
    if not os.path.exists(ui_summary_file):
        return

    with open(ui_summary_file, 'r', encoding='utf-8') as f:
        ui_data = json.load(f)

    texts = ui_data.get('all_unique_texts', [])

    # åˆ›å»ºåŠŸèƒ½å±‚çº§æ ‘
    hierarchy_file = os.path.join(hierarchy_dir, 'åŠŸèƒ½å±‚çº§æ ‘.md')
    with open(hierarchy_file, 'w', encoding='utf-8') as f:
        f.write("# åŠŸèƒ½å±‚çº§æ ‘\n\n")
        f.write("## ğŸŒ³ å®Œæ•´åŠŸèƒ½æ¶æ„\n\n")

        f.write("```\n")
        f.write("ğŸ“± alphaxæ±½è½¦å£°éŸ³æ§åˆ¶åº”ç”¨\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ ğŸš— è½¦è¾†ç®¡ç†\n")
        f.write("â”‚   â”œâ”€ è½¦è¾†é€‰æ‹©\n")
        f.write("â”‚   â”‚   â”œâ”€ æ¢è½¦\n")
        f.write("â”‚   â”‚   â”œâ”€ æˆ‘çš„è½¦\n")
        f.write("â”‚   â”‚   â””â”€ è½¦å‹ä¿¡æ¯\n")
        f.write("â”‚   â”‚       â”œâ”€ Mercedes-Benz\n")
        f.write("â”‚   â”‚       â”œâ”€ AMG ç³»åˆ—\n")
        f.write("â”‚   â”‚       â””â”€ Toyota Camry\n")
        f.write("â”‚   â””â”€ è½¦è¾†çŠ¶æ€\n")
        f.write("â”‚       â”œâ”€ å·²è¿æ¥\n")
        f.write("â”‚       â”œâ”€ ç¦»çº¿çŠ¶æ€\n")
        f.write("â”‚       â””â”€ æ‰«æä¸­\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ ğŸ”Š å£°éŸ³æ§åˆ¶\n")
        f.write("â”‚   â”œâ”€ å£°æµªè®¾ç½®\n")
        f.write("â”‚   â”‚   â”œâ”€ å£°æµªéŸ³é‡\n")
        f.write("â”‚   â”‚   â”œâ”€ å£°æµªéŸ³é¢‘\n")
        f.write("â”‚   â”‚   â””â”€ å£°æµªç±»å‹\n")
        f.write("â”‚   â”‚       â”œâ”€ è²æµª1\n")
        f.write("â”‚   â”‚       â”œâ”€ è²æµª2\n")
        f.write("â”‚   â”‚       â””â”€ è²æµª3\n")
        f.write("â”‚   â”œâ”€ å¯åŠ¨å£°éŸ³\n")
        f.write("â”‚   â”‚   â”œâ”€ å‹•æ…‹å•Ÿå‹•\n")
        f.write("â”‚   â”‚   â”œâ”€ å•Ÿå‹•å’†å“®\n")
        f.write("â”‚   â”‚   â””â”€ æ€ é€Ÿè²\n")
        f.write("â”‚   â””â”€ å£°éŸ³ä¸‹è½½\n")
        f.write("â”‚       â”œâ”€ éŸ³é¢‘èµ„æºä¸‹è½½\n")
        f.write("â”‚       â””â”€ æ­£åœ¨ä¸‹è½½çŠ¶æ€\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ ğŸ”— è¿æ¥ç®¡ç†\n")
        f.write("â”‚   â”œâ”€ è“ç‰™è¿æ¥\n")
        f.write("â”‚   â”œâ”€ è®¾å¤‡ç®¡ç†\n")
        f.write("â”‚   â”‚   â”œâ”€ è¾“å…¥æ–°è®¾å¤‡\n")
        f.write("â”‚   â”‚   â””â”€ è®¾å¤‡å·è¾“å…¥\n")
        f.write("â”‚   â””â”€ è¿æ¥çŠ¶æ€\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ ğŸ‘¤ ç”¨æˆ·ç®¡ç†\n")
        f.write("â”‚   â”œâ”€ ç™»å½•ç³»ç»Ÿ\n")
        f.write("â”‚   â”œâ”€ æˆ‘çš„è´¦æˆ·\n")
        f.write("â”‚   â””â”€ ç™»å‡ºè®¾å¤‡\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ âš™ï¸ ç³»ç»Ÿè®¾ç½®\n")
        f.write("â”‚   â”œâ”€ å¸¸ç”¨åŠŸèƒ½\n")
        f.write("â”‚   â”œâ”€ æ›´æ–°è½¯ä»¶\n")
        f.write("â”‚   â”œâ”€ æ£€æŸ¥æ›´æ–°\n")
        f.write("â”‚   â”œâ”€ è½¦è¾†å›ºä»¶\n")
        f.write("â”‚   â””â”€ æ¼”ç¤ºæ¨¡å¼\n")
        f.write("â”‚\n")
        f.write("â”œâ”€ ğŸ†˜ å¸®åŠ©æ”¯æŒ\n")
        f.write("â”‚   â”œâ”€ æŠ€æœ¯æ”¯æŒ\n")
        f.write("â”‚   â””â”€ æç¤ºå¼¹æ¡†\n")
        f.write("â”‚       â”œâ”€ æç¤ºå¼¹æ¡†1\n")
        f.write("â”‚       â””â”€ æç¤ºå¼¹æ¡†2\n")
        f.write("â”‚\n")
        f.write("â””â”€ ğŸ§­ å¯¼èˆªç•Œé¢\n")
        f.write("    â”œâ”€ é¦–é¡µ\n")
        f.write("    â”œâ”€ æˆ‘çš„\n")
        f.write("    â””â”€ è®¾ç½®\n")
        f.write("```\n\n")

        f.write("## ğŸ“Š åŠŸèƒ½å®Œæ•´åº¦ç»Ÿè®¡\n\n")

        # ç»Ÿè®¡æ¯ä¸ªåŠŸèƒ½æ¨¡å—çš„å…ƒç´ æ•°é‡
        function_stats = {
            'è½¦è¾†ç®¡ç†': 0,
            'å£°éŸ³æ§åˆ¶': 0,
            'è¿æ¥ç®¡ç†': 0,
            'ç”¨æˆ·ç®¡ç†': 0,
            'ç³»ç»Ÿè®¾ç½®': 0,
            'å¸®åŠ©æ”¯æŒ': 0,
            'å¯¼èˆªç•Œé¢': 0
        }

        for text_item in texts:
            text = text_item['text']
            if any(keyword in text for keyword in ['è½¦', 'æ¢è½¦', 'mercedes', 'amg', 'camry']):
                function_stats['è½¦è¾†ç®¡ç†'] += 1
            elif any(keyword in text for keyword in ['å£°', 'éŸ³', 'è²æµª', 'å¯åŠ¨', 'æ€ é€Ÿ']):
                function_stats['å£°éŸ³æ§åˆ¶'] += 1
            elif any(keyword in text for keyword in ['è“ç‰™', 'è¿æ¥', 'è®¾å¤‡', 'è¾“å…¥']):
                function_stats['è¿æ¥ç®¡ç†'] += 1
            elif any(keyword in text for keyword in ['ç™»å½•', 'æˆ‘çš„', 'ç™»å‡º']):
                function_stats['ç”¨æˆ·ç®¡ç†'] += 1
            elif any(keyword in text for keyword in ['è®¾ç½®', 'æ›´æ–°', 'å›ºä»¶', 'æ¼”ç¤º']):
                function_stats['ç³»ç»Ÿè®¾ç½®'] += 1
            elif any(keyword in text for keyword in ['æ”¯æŒ', 'æç¤º', 'å¼¹æ¡†']):
                function_stats['å¸®åŠ©æ”¯æŒ'] += 1
            elif any(keyword in text for keyword in ['é¦–é¡µ', 'å¯¼èˆª']):
                function_stats['å¯¼èˆªç•Œé¢'] += 1

        for function, count in function_stats.items():
            f.write(f"- **{function}**: {count} ä¸ªç›¸å…³å…ƒç´ \n")


def extract_ui_elements_from_large_content(content, all_ui_elements, all_texts, page_name):
    """ä»å¤§æ–‡ä»¶å†…å®¹ä¸­æå–UIå…ƒç´ """

    # æå–æ‰€æœ‰æ–‡å­—å†…å®¹
    text_patterns = [
        r'"string":\s*"([^"]+)"',
        r'"name":\s*"([^"]*[\u4e00-\u9fa5]+[^"]*)"',  # åŒ…å«ä¸­æ–‡çš„åç§°
        r'"attributedString"[^{}]*"string":\s*"([^"]+)"'
    ]

    for pattern in text_patterns:
        matches = re.finditer(pattern, content)
        for match in matches:
            text_content = match.group(1)
            if text_content and len(text_content.strip()) > 0:
                all_texts.append({
                    'text': text_content,
                    'source': page_name,
                    'type': 'text_content'
                })

    # æå–UIå…ƒç´ ç±»å‹
    ui_patterns = {
        'buttons': r'"_class":\s*".*button.*"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
        'groups': r'"_class":\s*"group"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
        'texts': r'"_class":\s*"text"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
        'rectangles': r'"_class":\s*"rectangle"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
        'shapes': r'"_class":\s*"shape.*"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
        'symbols': r'"_class":\s*"symbol.*"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}',
        'images': r'"_class":\s*".*image.*"[^{}]*{[^{}]*(?:{[^{}]*}[^{}]*)*}'
    }

    for element_type, pattern in ui_patterns.items():
        matches = re.finditer(pattern, content, re.IGNORECASE)
        for i, match in enumerate(matches):
            try:
                element_text = match.group()
                # å°è¯•æ„å»ºå®Œæ•´çš„JSON
                if not element_text.startswith('{'):
                    element_text = '{' + element_text
                if not element_text.endswith('}'):
                    element_text = element_text + '}'

                element_data = json.loads(element_text)
                element_info = {
                    'id': f"{page_name}_{element_type}_{i}",
                    'source': page_name,
                    'name': element_data.get('name', 'unnamed'),
                    'class': element_data.get('_class'),
                    'data': element_data
                }

                all_ui_elements[element_type].append(element_info)

            except json.JSONDecodeError:
                continue


def extract_ui_elements_recursive(data, all_ui_elements, all_texts, page_name, path="root"):
    """é€’å½’æå–UIå…ƒç´ """

    if isinstance(data, dict):
        # æ£€æŸ¥æ˜¯å¦æ˜¯UIå…ƒç´ 
        element_class = data.get('_class', '')
        element_name = data.get('name', '')
        element_id = data.get('do_objectID', f"unknown_{len(all_ui_elements.get('shapes', []))}")

        # åˆ†ç±»å¤„ç†ä¸åŒç±»å‹çš„å…ƒç´ 
        if element_class == 'text':
            # æå–æ–‡å­—å…ƒç´ 
            text_info = {
                'id': element_id,
                'name': element_name,
                'source': page_name,
                'path': path,
                'class': element_class,
                'frame': data.get('frame', {}),
                'data': data
            }

            # æå–æ–‡å­—å†…å®¹
            if 'attributedString' in data:
                attr_string = data['attributedString']
                if isinstance(attr_string, dict) and 'string' in attr_string:
                    text_content = attr_string['string']
                    all_texts.append({
                        'text': text_content,
                        'name': element_name,
                        'source': page_name,
                        'type': 'text_element',
                        'id': element_id
                    })

            all_ui_elements['texts'].append(text_info)

        elif 'button' in element_class.lower() or 'button' in element_name.lower():
            all_ui_elements['buttons'].append({
                'id': element_id,
                'name': element_name,
                'source': page_name,
                'path': path,
                'class': element_class,
                'frame': data.get('frame', {}),
                'data': data
            })

        elif element_class == 'group':
            all_ui_elements['groups'].append({
                'id': element_id,
                'name': element_name,
                'source': page_name,
                'path': path,
                'class': element_class,
                'frame': data.get('frame', {}),
                'layers_count': len(data.get('layers', [])),
                'data': data
            })

        elif element_class == 'rectangle':
            all_ui_elements['rectangles'].append({
                'id': element_id,
                'name': element_name,
                'source': page_name,
                'path': path,
                'class': element_class,
                'frame': data.get('frame', {}),
                'data': data
            })

        elif 'symbol' in element_class.lower():
            all_ui_elements['symbols'].append({
                'id': element_id,
                'name': element_name,
                'source': page_name,
                'path': path,
                'class': element_class,
                'frame': data.get('frame', {}),
                'data': data
            })

        elif 'shape' in element_class.lower() or 'path' in element_class.lower():
            category = 'paths' if 'path' in element_class.lower() else 'shapes'
            all_ui_elements[category].append({
                'id': element_id,
                'name': element_name,
                'source': page_name,
                'path': path,
                'class': element_class,
                'frame': data.get('frame', {}),
                'data': data
            })

        # æ£€æŸ¥å­—ç¬¦ä¸²å€¼ä¸­çš„æ–‡å­—å†…å®¹
        if 'string' in data and isinstance(data['string'], str):
            text_content = data['string']
            if text_content and len(text_content.strip()) > 0:
                all_texts.append({
                    'text': text_content,
                    'name': element_name or 'unnamed',
                    'source': page_name,
                    'type': 'string_value',
                    'id': element_id
                })

        # é€’å½’å¤„ç†å­å…ƒç´ 
        for key, value in data.items():
            if key not in ['data']:  # é¿å…é‡å¤å¤„ç†
                extract_ui_elements_recursive(value, all_ui_elements, all_texts, page_name, f"{path}.{key}")

    elif isinstance(data, list):
        for i, item in enumerate(data):
            extract_ui_elements_recursive(item, all_ui_elements, all_texts, page_name, f"{path}[{i}]")


def save_ui_elements_analysis(ui_elements_dir, all_ui_elements, all_texts):
    """ä¿å­˜UIå…ƒç´ åˆ†æç»“æœ"""

    # ä¿å­˜æ‰€æœ‰æ–‡å­—å†…å®¹
    texts_by_type = {}
    for text_item in all_texts:
        text_type = text_item['type']
        if text_type not in texts_by_type:
            texts_by_type[text_type] = []
        texts_by_type[text_type].append(text_item)

    with open(os.path.join(ui_elements_dir, 'all_texts.json'), 'w', encoding='utf-8') as f:
        json.dump({
            'total_texts': len(all_texts),
            'texts_by_type': texts_by_type,
            'all_texts': all_texts
        }, f, ensure_ascii=False, indent=2)

    # ç”Ÿæˆæ–‡å­—å†…å®¹æ‘˜è¦
    unique_texts = list(set([t['text'] for t in all_texts if t['text']]))
    with open(os.path.join(ui_elements_dir, 'texts_summary.txt'), 'w', encoding='utf-8') as f:
        f.write("=== æ‰€æœ‰æå–çš„æ–‡å­—å†…å®¹ ===\n\n")
        for i, text in enumerate(sorted(unique_texts), 1):
            f.write(f"{i:3d}. {text}\n")

    # ä¿å­˜å„ç±»UIå…ƒç´ 
    for element_type, elements in all_ui_elements.items():
        if elements:
            filename = f'{element_type}_elements.json'
            with open(os.path.join(ui_elements_dir, filename), 'w', encoding='utf-8') as f:
                json.dump({
                    'total_count': len(elements),
                    'elements': elements
                }, f, ensure_ascii=False, indent=2)

    # ç”ŸæˆUIå…ƒç´ æ‘˜è¦
    summary = {
        'extraction_summary': {
            'total_texts': len(all_texts),
            'unique_texts': len(unique_texts),
            'ui_elements_by_type': {k: len(v) for k, v in all_ui_elements.items()},
            'total_ui_elements': sum(len(v) for v in all_ui_elements.values())
        },
        'text_samples': unique_texts[:50],  # æ˜¾ç¤ºå‰50ä¸ªæ–‡å­—æ ·æœ¬
        'ui_element_samples': {
            element_type: [{'name': e.get('name'), 'class': e.get('class')}
                          for e in elements[:10]]  # æ¯ç±»æ˜¾ç¤ºå‰10ä¸ª
            for element_type, elements in all_ui_elements.items() if elements
        }
    }

    with open(os.path.join(ui_elements_dir, 'ui_analysis_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"    - æ–‡å­—æ‘˜è¦æ–‡ä»¶: texts_summary.txt ({len(unique_texts)} ä¸ªä¸é‡å¤æ–‡å­—)")


def generate_comprehensive_report(output_dir):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    report_file = os.path.join(output_dir, 'å®Œæ•´æå–æŠ¥å‘Š.md')

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# alphaxç•Œé¢è®¾è®¡.sketch å®Œæ•´æå–æŠ¥å‘Š\n\n")

        # ç»Ÿè®¡ä¿¡æ¯
        f.write("## ğŸ“Š æå–ç»Ÿè®¡\n\n")

        # å›¾ç‰‡ç»Ÿè®¡
        images_count = len(list(Path(output_dir).glob('00_æ‰€æœ‰å›¾ç‰‡/*')))
        f.write(f"- å›¾ç‰‡æ–‡ä»¶: {images_count} ä¸ª\n")

        # ç¬¦å·ç»Ÿè®¡
        symbols_file = os.path.join(output_dir, '01_ç¬¦å·åº“_Symbols', 'symbols_info.json')
        if os.path.exists(symbols_file):
            with open(symbols_file, 'r', encoding='utf-8') as sf:
                symbols_data = json.load(sf)
                f.write(f"- ç¬¦å·æ•°é‡: {symbols_data.get('symbols_count', 0)} ä¸ª\n")

        # ç”»æ¿ç»Ÿè®¡
        artboard_summary_file = os.path.join(output_dir, '02_ç”»æ¿_Artboards', 'artboards_summary.json')
        if os.path.exists(artboard_summary_file):
            with open(artboard_summary_file, 'r', encoding='utf-8') as asf:
                artboard_data = json.load(asf)
                f.write(f"- ç”»æ¿æ•°é‡: {artboard_data.get('total_artboards', 0)} ä¸ª\n")
        else:
            f.write("- ç”»æ¿æ•°é‡: 0 ä¸ª\n")

        # é¡µé¢ç»Ÿè®¡
        pages_src_dir = os.path.join(output_dir, 'pages')
        if os.path.exists(pages_src_dir):
            pages_count = len([f for f in os.listdir(pages_src_dir) if f.endswith('.json')])
            f.write(f"- é¡µé¢æ•°é‡: {pages_count} ä¸ª\n")

        # æ’ä»¶ç»Ÿè®¡
        plugin_summary_file = os.path.join(output_dir, '05_æ’ä»¶æ•°æ®', 'plugin_summary.json')
        if os.path.exists(plugin_summary_file):
            with open(plugin_summary_file, 'r', encoding='utf-8') as psf:
                plugin_data = json.load(psf)
                f.write(f"- å°å‹ç»„ä»¶: {len(plugin_data.get('mini_components', []))} ä¸ª\n")
                f.write(f"- å­—ä½“æ–‡ä»¶: {len(plugin_data.get('fonts', []))} ä¸ª\n")

        # UIå…ƒç´ ç»Ÿè®¡
        ui_summary_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'ui_analysis_summary.json')
        if os.path.exists(ui_summary_file):
            with open(ui_summary_file, 'r', encoding='utf-8') as usf:
                ui_data = json.load(usf)
                extraction_summary = ui_data.get('extraction_summary', {})
                f.write(f"- æ‰€æœ‰æ–‡å­—: {extraction_summary.get('total_texts', 0)} ä¸ª\n")
                f.write(f"- ä¸é‡å¤æ–‡å­—: {extraction_summary.get('unique_texts', 0)} ä¸ª\n")
                f.write(f"- UIå…ƒç´ : {extraction_summary.get('total_ui_elements', 0)} ä¸ª\n")
        f.write("\n")

        f.write("## ğŸ“ ç›®å½•ç»“æ„\n\n")
        f.write("```\n")
        for item in sorted(os.listdir(output_dir)):
            if os.path.isdir(os.path.join(output_dir, item)):
                f.write(f"{item}/\n")
                # æ˜¾ç¤ºå­ç›®å½•å†…å®¹
                subdir = os.path.join(output_dir, item)
                for subitem in sorted(os.listdir(subdir))[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    f.write(f"  â”œâ”€â”€ {subitem}\n")
                if len(os.listdir(subdir)) > 5:
                    f.write(f"  â””â”€â”€ ... è¿˜æœ‰ {len(os.listdir(subdir)) - 5} ä¸ªæ–‡ä»¶\n")
            else:
                f.write(f"{item}\n")
        f.write("```\n\n")

        f.write("## ğŸ¯ æ–°åŠŸèƒ½ç‰¹ç‚¹\n\n")
        f.write("âœ… **æ·±åº¦é€’å½’è§£æ** - å®Œæ•´éå†æ‰€æœ‰å±‚çº§ç»“æ„\n")
        f.write("âœ… **åµŒå…¥å›¾ç‰‡æå–** - æ”¯æŒbase64å’Œå¼•ç”¨å›¾ç‰‡\n")
        f.write("âœ… **å¤§æ–‡ä»¶å¤„ç†** - æ™ºèƒ½åˆ†å—å¤„ç†å¤§å‹JSON\n")
        f.write("âœ… **å°å‹ç»„ä»¶è¯†åˆ«** - è‡ªåŠ¨è¯†åˆ«æŒ‰é’®ã€å›¾æ ‡ç­‰ç»„ä»¶\n")
        f.write("âœ… **ç¬¦å·å®Œæ•´æå–** - ä¿®å¤ç¬¦å·å’Œç»„ä»¶çš„æå–é€»è¾‘\n\n")

        f.write("## ğŸ”§ ä½¿ç”¨è¯´æ˜\n\n")
        f.write("1. **00_æ‰€æœ‰å›¾ç‰‡**: åŒ…å«æ‰€æœ‰å›¾ç‰‡èµ„æº\n")
        f.write("   - `embedded/` - åµŒå…¥çš„base64å›¾ç‰‡\n")
        f.write("   - `references/` - å¼•ç”¨çš„å›¾ç‰‡èµ„æº\n")
        f.write("2. **01_ç¬¦å·åº“_Symbols**: ç¬¦å·å’Œç»„ä»¶å®Œæ•´æ•°æ®\n")
        f.write("   - `all_symbols.json` - æ‰€æœ‰æ‰¾åˆ°çš„ç¬¦å·\n")
        f.write("   - `all_components.json` - æŒ‰ç±»å‹åˆ†ç±»çš„ç»„ä»¶\n")
        f.write("3. **02_ç”»æ¿_Artboards**: å®Œæ•´çš„ç”»æ¿ä¿¡æ¯\n")
        f.write("   - `artboards_summary.json` - ç”»æ¿ç»Ÿè®¡æ‘˜è¦\n")
        f.write("4. **03_é¡µé¢_Pages**: é¡µé¢ç»“æ„å’Œæ–‡æ¡£ä¿¡æ¯\n")
        f.write("5. **04_æ ·å¼_Styles**: å›¾å±‚æ ·å¼å’Œæ–‡æœ¬æ ·å¼\n")
        f.write("6. **05_æ’ä»¶æ•°æ®**: å®Œæ•´çš„æ’ä»¶å’Œå°å‹ç»„ä»¶\n")
        f.write("   - `mini_components/` - è¯†åˆ«å‡ºçš„å°å‹UIç»„ä»¶\n")
        f.write("   - `plugin_summary.json` - æ’ä»¶æ•°æ®æ€»ç»“\n")
        f.write("   - `fonts/` - æå–çš„å­—ä½“æ–‡ä»¶\n")
        f.write("7. **06_åŸå§‹æ–‡ä»¶**: åŸå§‹JSONæ–‡ä»¶å¤‡ä»½\n")
        f.write("8. **08_UIå…ƒç´ è¯¦ç»†åˆ†æ**: å®Œæ•´çš„UIå…ƒç´ å’Œæ–‡å­—åˆ†æ\n")
        f.write("   - `all_texts.json` - æ‰€æœ‰æå–çš„æ–‡å­—å†…å®¹\n")
        f.write("   - `texts_summary.txt` - æ–‡å­—å†…å®¹æ¸…å•\n")
        f.write("   - `*_elements.json` - å„ç±»å‹UIå…ƒç´ è¯¦ç»†æ•°æ®\n")
        f.write("   - `ui_analysis_summary.json` - UIå…ƒç´ åˆ†ææ€»ç»“\n")


def extract_embedded_images_from_json(output_dir, embedded_images_dir):
    """ä»JSONæ–‡ä»¶ä¸­æå–åµŒå…¥çš„base64å›¾ç‰‡å’Œå›¾ç‰‡å¼•ç”¨"""
    print("  - æœç´¢åµŒå…¥å›¾ç‰‡å’Œbase64ç¼–ç å›¾ç‰‡...")
    image_counter = 1

    for root, dirs, files in os.walk(output_dir):
        for file in files:
            if file.endswith('.json'):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                        # æŸ¥æ‰¾base64ç¼–ç çš„å›¾ç‰‡
                        base64_patterns = [
                            r'data:image/([^;]+);base64,([A-Za-z0-9+/=]+)',
                            r'"image":\s*"([A-Za-z0-9+/=]{100,})"',
                            r'"data":\s*"([A-Za-z0-9+/=]{100,})"'
                        ]

                        for pattern in base64_patterns:
                            matches = re.findall(pattern, content)
                            for match in matches:
                                if isinstance(match, tuple):
                                    if len(match) == 2:  # data:image format
                                        format_type, base64_data = match
                                        extension = format_type.split('/')[1] if '/' in format_type else 'png'
                                    else:
                                        base64_data = match[0]
                                        extension = 'png'
                                else:
                                    base64_data = match
                                    extension = 'png'

                                try:
                                    image_data = base64.b64decode(base64_data)
                                    if len(image_data) > 100:  # è¿‡æ»¤æ‰å¤ªå°çš„æ•°æ®
                                        filename = f"embedded_{image_counter}.{extension}"
                                        with open(os.path.join(embedded_images_dir, filename), 'wb') as img_file:
                                            img_file.write(image_data)
                                        print(f"    * æå–åµŒå…¥å›¾ç‰‡: {filename} ({len(image_data)} bytes)")
                                        image_counter += 1
                                except Exception:
                                    continue

                        # è§£æJSONæŸ¥æ‰¾å›¾ç‰‡å¼•ç”¨
                        try:
                            json_data = json.loads(content)
                            extract_image_refs_from_obj(json_data, embedded_images_dir, output_dir, image_counter)
                        except json.JSONDecodeError:
                            continue

                except Exception as e:
                    continue


def extract_image_refs_from_obj(obj, embedded_images_dir, output_dir, counter=1):
    """é€’å½’æœç´¢å¯¹è±¡ä¸­çš„å›¾ç‰‡å¼•ç”¨"""
    if isinstance(obj, dict):
        for key, value in obj.items():
            if key in ['image', 'fill', 'border', 'shadow'] and isinstance(value, dict):
                # æŸ¥æ‰¾å›¾ç‰‡å¼•ç”¨
                if '_ref' in value and 'images/' in str(value.get('_ref', '')):
                    ref_path = value['_ref']
                    source_path = os.path.join(output_dir, ref_path)
                    if os.path.exists(source_path):
                        filename = f"ref_{counter}_{os.path.basename(ref_path)}"
                        dest_path = os.path.join(embedded_images_dir, filename)
                        if not os.path.exists(dest_path):
                            shutil.copy2(source_path, dest_path)
                            counter += 1
            else:
                extract_image_refs_from_obj(value, embedded_images_dir, output_dir, counter)
    elif isinstance(obj, list):
        for item in obj:
            extract_image_refs_from_obj(item, embedded_images_dir, output_dir, counter)


def extract_image_references(output_dir, images_dir):
    """æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡å¼•ç”¨"""
    print("  - æœç´¢å›¾ç‰‡å¼•ç”¨...")
    refs_dir = os.path.join(images_dir, 'references')
    os.makedirs(refs_dir, exist_ok=True)

    # æœç´¢æ‰€æœ‰å¯èƒ½åŒ…å«å›¾ç‰‡å¼•ç”¨çš„æ¨¡å¼
    image_patterns = [
        r'([a-f0-9]{40})\.(png|jpg|jpeg|gif|webp|svg)',  # SHA1å“ˆå¸Œå‘½åçš„å›¾ç‰‡
        r'images/([^"\'}\s]+\.(png|jpg|jpeg|gif|webp|svg))',  # images/ç›®å½•å¼•ç”¨
        r'"image":\s*"([^"]+\.(png|jpg|jpeg|gif|webp|svg))"',  # JSONä¸­çš„å›¾ç‰‡å­—æ®µ
    ]

    found_images = set()

    for root, dirs, files in os.walk(output_dir):
        for file in files:
            if file.endswith(('.json', '.txt', '.xml')):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        for pattern in image_patterns:
                            matches = re.findall(pattern, content)
                            for match in matches:
                                if isinstance(match, tuple):
                                    image_name = match[0] + '.' + match[1] if len(match) > 1 else match[0]
                                else:
                                    image_name = match
                                found_images.add(image_name)
                except Exception:
                    continue

    # å°è¯•æ‰¾åˆ°å¹¶å¤åˆ¶è¿™äº›å›¾ç‰‡
    for image_name in found_images:
        for root, dirs, files in os.walk(output_dir):
            for file in files:
                if file == os.path.basename(image_name) or file.endswith(os.path.basename(image_name)):
                    source_path = os.path.join(root, file)
                    dest_path = os.path.join(refs_dir, file)
                    if not os.path.exists(dest_path) and 'references' not in source_path:
                        shutil.copy2(source_path, dest_path)
                        print(f"    * æ‰¾åˆ°å¼•ç”¨å›¾ç‰‡: {file}")


def extract_symbols_from_pages(output_dir, symbols_dir, all_symbols, all_components):
    """ä»é¡µé¢ä¸­æ·±åº¦æå–ç¬¦å·å’Œç»„ä»¶"""
    print("  - æ·±åº¦è§£æé¡µé¢ä¸­çš„ç¬¦å·...")
    pages_dir = os.path.join(output_dir, 'pages')

    if not os.path.exists(pages_dir):
        return

    for page_file in os.listdir(pages_dir):
        if page_file.endswith('.json'):
            page_path = os.path.join(pages_dir, page_file)
            try:
                with open(page_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # åˆ†å—è¯»å–å¤§æ–‡ä»¶
                if len(content) > 1000000:  # 1MB
                    process_large_json_file(page_path, symbols_dir, all_symbols, all_components)
                else:
                    page_data = json.loads(content)
                    extract_symbols_from_data(page_data, symbols_dir, all_symbols, all_components, page_file)

            except Exception as e:
                print(f"    ! å¤„ç†é¡µé¢ {page_file} æ—¶å‡ºé”™: {e}")
                continue

    # ä¿å­˜æ‰€æœ‰æ‰¾åˆ°çš„ç¬¦å·
    if all_symbols:
        with open(os.path.join(symbols_dir, 'all_symbols.json'), 'w', encoding='utf-8') as f:
            json.dump(all_symbols, f, ensure_ascii=False, indent=2)

    if all_components:
        with open(os.path.join(symbols_dir, 'all_components.json'), 'w', encoding='utf-8') as f:
            json.dump(dict(all_components), f, ensure_ascii=False, indent=2)


def process_large_json_file(file_path, symbols_dir, all_symbols, all_components):
    """å¤„ç†å¤§å‹JSONæ–‡ä»¶ï¼Œåˆ†å—è¯»å–"""
    print(f"    - å¤„ç†å¤§æ–‡ä»¶: {os.path.basename(file_path)}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # å°è¯•æµå¼è§£æ
            content = f.read()

            # æŸ¥æ‰¾ç¬¦å·ç›¸å…³çš„å…³é”®å­—
            symbol_keywords = ['symbol', 'component', 'master', 'instance']
            for keyword in symbol_keywords:
                pattern = rf'"{keyword}"[^}}]*}}'
                matches = re.finditer(pattern, content, re.IGNORECASE)
                for i, match in enumerate(matches):
                    try:
                        symbol_data = json.loads('{' + match.group() + '}')
                        all_symbols[f'{keyword}_{i}'] = symbol_data
                    except json.JSONDecodeError:
                        continue

    except Exception as e:
        print(f"      ! å¤„ç†å¤§æ–‡ä»¶æ—¶å‡ºé”™: {e}")


def extract_symbols_from_data(data, symbols_dir, all_symbols, all_components, source_file):
    """ä»æ•°æ®ä¸­é€’å½’æå–ç¬¦å·"""
    def recursive_extract(obj, path="root"):
        if isinstance(obj, dict):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¦å·å®šä¹‰
            if '_class' in obj:
                class_type = obj['_class']
                if 'symbol' in class_type.lower() or 'component' in class_type.lower():
                    symbol_id = obj.get('do_objectID', f"unknown_{len(all_symbols)}")
                    all_symbols[symbol_id] = obj
                    print(f"      * æ‰¾åˆ°ç¬¦å·: {class_type} - {obj.get('name', 'unnamed')}")

                if 'artboard' in class_type.lower():
                    artboard_name = obj.get('name', f'artboard_{len(all_components["artboards"])}')
                    all_components['artboards'].append({
                        'name': artboard_name,
                        'source': source_file,
                        'data': obj
                    })

            # é€’å½’æœç´¢
            for key, value in obj.items():
                recursive_extract(value, f"{path}.{key}")
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                recursive_extract(item, f"{path}[{i}]")

    recursive_extract(data)


def create_unique_filename(directory, filename):
    """åˆ›å»ºå”¯ä¸€çš„æ–‡ä»¶å"""
    base, ext = os.path.splitext(filename)
    counter = 1
    new_filename = filename

    while os.path.exists(os.path.join(directory, new_filename)):
        new_filename = f"{base}_{counter}{ext}"
        counter += 1

    return new_filename


def print_summary(output_dir):
    """æ‰“å°æå–æ‘˜è¦"""
    print(f"ğŸ“ å®Œæ•´è¾“å‡ºç›®å½•: {output_dir}")

    # å›¾ç‰‡ç»Ÿè®¡
    total_images = len(list(Path(output_dir).glob('00_æ‰€æœ‰å›¾ç‰‡/**/*')))
    embedded_images = len(list(Path(output_dir).glob('00_æ‰€æœ‰å›¾ç‰‡/embedded/*')))
    ref_images = len(list(Path(output_dir).glob('00_æ‰€æœ‰å›¾ç‰‡/references/*')))
    print(f"ğŸ–¼ï¸  å›¾ç‰‡èµ„æº: {total_images} å¼  (åµŒå…¥:{embedded_images}, å¼•ç”¨:{ref_images})")

    # ç¬¦å·ç»Ÿè®¡
    all_symbols_file = os.path.join(output_dir, '01_ç¬¦å·åº“_Symbols', 'all_symbols.json')
    if os.path.exists(all_symbols_file):
        with open(all_symbols_file, 'r', encoding='utf-8') as f:
            symbols_data = json.load(f)
            print(f"ğŸ”§ ç¬¦å·ç»„ä»¶: {len(symbols_data)} ä¸ª")

    # ç”»æ¿ç»Ÿè®¡
    artboard_summary_file = os.path.join(output_dir, '02_ç”»æ¿_Artboards', 'artboards_summary.json')
    if os.path.exists(artboard_summary_file):
        with open(artboard_summary_file, 'r', encoding='utf-8') as f:
            artboard_data = json.load(f)
            print(f"ğŸ¯ ç”»æ¿æ•°é‡: {artboard_data.get('total_artboards', 0)} ä¸ª")

    # æ’ä»¶å’Œå°å‹ç»„ä»¶ç»Ÿè®¡
    plugin_summary_file = os.path.join(output_dir, '05_æ’ä»¶æ•°æ®', 'plugin_summary.json')
    if os.path.exists(plugin_summary_file):
        with open(plugin_summary_file, 'r', encoding='utf-8') as f:
            plugin_data = json.load(f)
            mini_components = len(plugin_data.get('mini_components', []))
            fonts = len(plugin_data.get('fonts', []))
            print(f"ğŸ”Œ å°å‹ç»„ä»¶: {mini_components} ä¸ª")
            print(f"ğŸ“ å­—ä½“æ–‡ä»¶: {fonts} ä¸ª")

    # UIå…ƒç´ å’Œæ–‡å­—ç»Ÿè®¡
    ui_summary_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'ui_analysis_summary.json')
    if os.path.exists(ui_summary_file):
        with open(ui_summary_file, 'r', encoding='utf-8') as f:
            ui_data = json.load(f)
            extraction_summary = ui_data.get('extraction_summary', {})
            ui_elements = extraction_summary.get('ui_elements_by_type', {})
            print(f"ğŸ“ æ‰€æœ‰æ–‡å­—: {extraction_summary.get('total_texts', 0)} ä¸ª (ä¸é‡å¤: {extraction_summary.get('unique_texts', 0)} ä¸ª)")
            print(f"ğŸ¨ UIå…ƒç´ : ç»„{ui_elements.get('groups', 0)}ä¸ª, æŒ‰é’®{ui_elements.get('buttons', 0)}ä¸ª, çŸ©å½¢{ui_elements.get('rectangles', 0)}ä¸ª")

    print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Š: æŸ¥çœ‹ '{output_dir}/å®Œæ•´æå–æŠ¥å‘Š.md'")


def main():
    """ä¸»å‡½æ•°"""
    print("alphaxç•Œé¢è®¾è®¡.sketch å®Œæ•´èµ„æºæå–å·¥å…·")
    print("ç‰ˆæœ¬: 3.0 - å®Œæ•´æ·±åº¦æå–ï¼Œæ”¯æŒå¤§æ–‡ä»¶å¤„ç†")
    print("=" * 60)

    if extract_sketch_completely():
        print("\nâœ¨ æ‰€æœ‰è®¾è®¡èµ„æºæå–å®Œæˆï¼")
        print("ğŸ’¡ ç°åœ¨æ‚¨å¯ä»¥æŸ¥çœ‹ç¬¦å·ã€ç»„ä»¶ã€ç”»æ¿ç­‰å®Œæ•´è®¾è®¡å…ƒç´ ")
    else:
        print("\nâŒ æå–å¤±è´¥")


def export_hierarchical_png_elements(output_dir):
    """æŒ‰å±‚çº§ç»“æ„å¯¼å‡ºPNGå…ƒç´ åˆ°åˆ†ç±»ç›®å½•"""

    # åˆ›å»ºå±‚çº§PNGç›®å½•
    png_hierarchy_dir = os.path.join(output_dir, '10_PNGå…ƒç´ åˆ†å±‚å¯¼å‡º')
    if os.path.exists(png_hierarchy_dir):
        shutil.rmtree(png_hierarchy_dir)
    os.makedirs(png_hierarchy_dir)

    # æ ¹æ®åŠŸèƒ½å±‚çº§åˆ›å»ºç›®å½•ç»“æ„
    hierarchy_structure = {
        '01_è½¦è¾†ç®¡ç†': {
            'description': 'è½¦è¾†é€‰æ‹©å’ŒçŠ¶æ€ç›¸å…³PNG',
            'keywords': ['è½¦', 'æ¢è½¦', 'Camry', 'Mercedes', 'AMG', 'å·²é€£æ¥', 'offline', 'Scanning'],
            'subdirs': {
                'è½¦è¾†é€‰æ‹©': ['æ¢è½¦', 'æˆ‘çš„è½¦', 'Camry', 'Mercedes', 'AMG'],
                'è½¦è¾†çŠ¶æ€': ['å·²é€£æ¥', 'offline', 'Scanning', 'æ­£åœ¨ä¸‹è¼‰']
            }
        },
        '02_å£°éŸ³æ§åˆ¶': {
            'description': 'å£°æµªè®¾ç½®å’ŒéŸ³é¢‘æ§åˆ¶PNG',
            'keywords': ['è²æµª', 'è²éŸ³', 'éŸ³é‡', 'ä¸‹è½½', 'å•Ÿå‹•', 'æ€ é€Ÿ', 'å‹•æ…‹'],
            'subdirs': {
                'å£°æµªè®¾ç½®': ['è²æµªè¨­ç½®', 'è²æµª1', 'è²æµª2', 'è²æµª3'],
                'å¯åŠ¨å£°éŸ³': ['å‹•æ…‹å•Ÿå‹•', 'å•Ÿå‹•å’†å“®', 'æ€ é€Ÿè²'],
                'éŸ³é¢‘ä¸‹è½½': ['å£°éŸ³ä¸‹è½½', 'éŸ³é‡ä¸‹è½½', 'æ­£åœ¨ä¸‹è¼‰è²éŸ³è³‡æº']
            }
        },
        '03_è¿æ¥ç®¡ç†': {
            'description': 'è“ç‰™è¿æ¥å’Œè®¾å¤‡ç®¡ç†PNG',
            'keywords': ['è“ç‰™', 'é“¾æ¥', 'è®¾å¤‡', 'è¯·è¾“å…¥'],
            'subdirs': {
                'è“ç‰™è¿æ¥': ['è“ç‰™é“¾æ¥'],
                'è®¾å¤‡ç®¡ç†': ['è¯·è¾“å…¥æ–°è®¾å¤‡', 'è¯·è¾“å…¥æ–°è®¾å¤‡å·']
            }
        },
        '04_ç”¨æˆ·ç®¡ç†': {
            'description': 'ç”¨æˆ·è´¦æˆ·å’Œç™»å½•ç›¸å…³PNG',
            'keywords': ['ç™»å½•', 'æˆ‘çš„', 'ç”¨æˆ·'],
            'subdirs': {
                'è´¦æˆ·ç®¡ç†': ['ç™»å½•', 'æˆ‘çš„'],
                'ç”¨æˆ·æ“ä½œ': ['ç™»å‡ºè®¾å¤‡']
            }
        },
        '05_ç³»ç»Ÿè®¾ç½®': {
            'description': 'ç³»ç»Ÿè®¾ç½®å’Œé…ç½®PNG',
            'keywords': ['è®¾ç½®', 'æ›´æ–°', 'å›ºä»¶', 'æ¼”ç¤º'],
            'subdirs': {
                'åŸºæœ¬è®¾ç½®': ['è®¾ç½®'],
                'ç³»ç»Ÿæ›´æ–°': ['æ›´æ–°è½¯ä»¶', 'æ£€æŸ¥æ›´æ–°', 'è½¦è¾†å›ºä»¶'],
                'ç‰¹æ®Šæ¨¡å¼': ['æ¼”ç¤ºæ¨¡å¼']
            }
        },
        '06_å¸®åŠ©æ”¯æŒ': {
            'description': 'æŠ€æœ¯æ”¯æŒå’Œå¸®åŠ©PNG',
            'keywords': ['æ”¯æŒ', 'æŠ€è¡“æ”¯æŒ', 'æç¤ºå¼¹æ¡†'],
            'subdirs': {
                'æŠ€æœ¯æ”¯æŒ': ['æ”¯æŒ', 'æŠ€è¡“æ”¯æŒ'],
                'æç¤ºä¿¡æ¯': ['æç¤ºå¼¹æ¡†1', 'æç¤ºå¼¹æ¡†2']
            }
        },
        '07_å¯¼èˆªç•Œé¢': {
            'description': 'åº”ç”¨å¯¼èˆªå’Œé¡µé¢PNG',
            'keywords': ['é¦–é¡µ', 'æˆ‘çš„', 'è®¾ç½®é¡µé¢'],
            'subdirs': {
                'ä¸»è¦é¡µé¢': ['é¦–é¡µ', 'æˆ‘çš„æ±½è»Š', 'æˆ‘çš„è²éŸ³'],
                'é¡µé¢çŠ¶æ€': ['é¦–é¡µ-ç‚¹å‡»å‘å£°æ ·å¼', 'é¦–é¡µ-è»Šè¼›å‹•æ…‹çš„æ–‡å­—']
            }
        },
        '08_æŒ‰é’®å…ƒç´ ': {
            'description': 'å„ç±»æŒ‰é’®PNGå…ƒç´ ',
            'keywords': ['ç¡®å®š', 'å®Œæˆ', 'å¥½çš„', 'å–æ¶ˆ'],
            'subdirs': {
                'ç¡®è®¤æŒ‰é’®': ['ç¡®å®š', 'å®Œæˆ', 'å¥½çš„'],
                'æ“ä½œæŒ‰é’®': ['ä¸‹è½½', 'è®¾ç½®', 'æ¢è½¦']
            }
        }
    }

    # è¯»å–UIåˆ†ææ•°æ®
    ui_analysis_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'ui_analysis_summary.json')
    all_texts_file = os.path.join(output_dir, '08_UIå…ƒç´ è¯¦ç»†åˆ†æ', 'all_texts.json')

    ui_elements = {}
    all_texts = {}

    try:
        if os.path.exists(ui_analysis_file):
            with open(ui_analysis_file, 'r', encoding='utf-8') as f:
                ui_elements = json.load(f)
    except:
        pass

    try:
        if os.path.exists(all_texts_file):
            with open(all_texts_file, 'r', encoding='utf-8') as f:
                all_texts = json.load(f)
    except:
        pass

    # åˆ›å»ºå±‚çº§ç›®å½•ç»“æ„
    for category, info in hierarchy_structure.items():
        category_dir = os.path.join(png_hierarchy_dir, category)
        os.makedirs(category_dir, exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        for subdir in info['subdirs']:
            subdir_path = os.path.join(category_dir, subdir)
            os.makedirs(subdir_path, exist_ok=True)

    # å¤åˆ¶ç°æœ‰PNGæ–‡ä»¶åˆ°å±‚çº§ç»“æ„ä¸­
    source_images_dir = os.path.join(output_dir, '00_æ‰€æœ‰å›¾ç‰‡')
    png_files = []

    if os.path.exists(source_images_dir):
        for root, dirs, files in os.walk(source_images_dir):
            for file in files:
                if file.lower().endswith('.png'):
                    png_files.append(os.path.join(root, file))

    # åˆ†ç±»PNGæ–‡ä»¶
    categorized_count = 0

    for png_file in png_files:
        filename = os.path.basename(png_file)
        file_copied = False

        # æ ¹æ®æ–‡ä»¶åå’Œå…³è”æ–‡æœ¬åˆ†ç±»
        for category, info in hierarchy_structure.items():
            category_dir = os.path.join(png_hierarchy_dir, category)

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…å…³é”®è¯
            for keyword in info['keywords']:
                if keyword.lower() in filename.lower() or any(keyword in text for text in all_texts.get('unique_texts', [])):
                    # è¿›ä¸€æ­¥åˆ†ç±»åˆ°å­ç›®å½•
                    for subdir, subdir_keywords in info['subdirs'].items():
                        for sub_keyword in subdir_keywords:
                            if sub_keyword.lower() in filename.lower() or any(sub_keyword in text for text in all_texts.get('unique_texts', [])):
                                target_dir = os.path.join(category_dir, subdir)
                                target_file = os.path.join(target_dir, f"{sub_keyword}_{filename}")
                                try:
                                    shutil.copy2(png_file, target_file)
                                    file_copied = True
                                    categorized_count += 1
                                    break
                                except:
                                    pass
                        if file_copied:
                            break

                    # å¦‚æœæ²¡æœ‰åˆ†åˆ°å­ç›®å½•ï¼Œæ”¾åˆ°ä¸»ç›®å½•
                    if not file_copied:
                        target_file = os.path.join(category_dir, f"{keyword}_{filename}")
                        try:
                            shutil.copy2(png_file, target_file)
                            file_copied = True
                            categorized_count += 1
                        except:
                            pass
                    break

            if file_copied:
                break

        # æœªåˆ†ç±»çš„æ–‡ä»¶æ”¾åˆ°é€šç”¨ç›®å½•
        if not file_copied:
            uncategorized_dir = os.path.join(png_hierarchy_dir, '09_æœªåˆ†ç±»PNG')
            os.makedirs(uncategorized_dir, exist_ok=True)
            target_file = os.path.join(uncategorized_dir, filename)
            try:
                shutil.copy2(png_file, target_file)
            except:
                pass

    # åˆ›å»ºè™šæ‹ŸPNGå…ƒç´ ï¼ˆåŸºäºæ–‡æœ¬å†…å®¹ï¼‰
    create_text_based_png_placeholders(png_hierarchy_dir, all_texts, hierarchy_structure)

    # ç”Ÿæˆå±‚çº§ç»“æ„è¯´æ˜æ–‡æ¡£
    create_png_hierarchy_documentation(png_hierarchy_dir, hierarchy_structure, categorized_count, len(png_files))

    print(f"    âœ… PNGå…ƒç´ æŒ‰å±‚çº§å¯¼å‡ºå®Œæˆ")
    print(f"    ğŸ“ æ€»è®¡: {len(png_files)} ä¸ªPNGæ–‡ä»¶")
    print(f"    ğŸ¯ å·²åˆ†ç±»: {categorized_count} ä¸ª")
    print(f"    ğŸ“‚ å¯¼å‡ºä½ç½®: {png_hierarchy_dir}")


def create_text_based_png_placeholders(png_hierarchy_dir, all_texts, hierarchy_structure):
    """åŸºäºæ–‡æœ¬å†…å®¹åˆ›å»ºPNGå…ƒç´ å ä½ç¬¦"""

    if not all_texts or 'unique_texts' not in all_texts:
        return

    # ä¸ºæ¯ä¸ªæ–‡æœ¬å…ƒç´ åˆ›å»ºå ä½è¯´æ˜
    for category, info in hierarchy_structure.items():
        category_dir = os.path.join(png_hierarchy_dir, category)

        for subdir, keywords in info['subdirs'].items():
            subdir_path = os.path.join(category_dir, subdir)

            # åˆ›å»ºè¯¥ç±»åˆ«çš„æ–‡æœ¬å…ƒç´ æ¸…å•
            matching_texts = []
            for text in all_texts['unique_texts']:
                for keyword in keywords:
                    if keyword in text:
                        matching_texts.append(text)
                        break

            if matching_texts:
                # åˆ›å»ºæ–‡æœ¬å…ƒç´ æ¸…å•æ–‡ä»¶
                text_list_file = os.path.join(subdir_path, '_æ–‡æœ¬å…ƒç´ æ¸…å•.md')
                with open(text_list_file, 'w', encoding='utf-8') as f:
                    f.write(f"# {subdir} - æ–‡æœ¬å…ƒç´ æ¸…å•\n\n")
                    f.write("ä»¥ä¸‹æ–‡æœ¬å…ƒç´ å¯ä»¥å¯¼å‡ºä¸ºPNGï¼š\n\n")
                    for i, text in enumerate(matching_texts, 1):
                        f.write(f"{i}. `{text}`\n")
                    f.write(f"\nğŸ’¡ ä»Sketchä¸­æ‹–æ‹½è¿™äº›å…ƒç´ å¯ç”Ÿæˆå¯¹åº”çš„PNGæ–‡ä»¶\n")


def create_png_hierarchy_documentation(png_hierarchy_dir, hierarchy_structure, categorized_count, total_count):
    """åˆ›å»ºPNGå±‚çº§ç»“æ„è¯´æ˜æ–‡æ¡£"""

    doc_file = os.path.join(png_hierarchy_dir, 'PNGå±‚çº§ç»“æ„è¯´æ˜.md')

    with open(doc_file, 'w', encoding='utf-8') as f:
        f.write("# PNGå…ƒç´ åˆ†å±‚å¯¼å‡ºè¯´æ˜\n\n")
        f.write("## ğŸ“Š å¯¼å‡ºç»Ÿè®¡\n\n")
        f.write(f"- æ€»PNGæ–‡ä»¶æ•°: {total_count}\n")
        f.write(f"- å·²åˆ†ç±»æ–‡ä»¶æ•°: {categorized_count}\n")
        f.write(f"- åˆ†ç±»å‡†ç¡®ç‡: {(categorized_count/total_count*100):.1f}%\n\n")

        f.write("## ğŸ—‚ï¸ ç›®å½•ç»“æ„\n\n")
        f.write("```\n")
        f.write("10_PNGå…ƒç´ åˆ†å±‚å¯¼å‡º/\n")

        for category, info in hierarchy_structure.items():
            f.write(f"â”œâ”€â”€ {category}/  # {info['description']}\n")
            for subdir in info['subdirs']:
                f.write(f"â”‚   â”œâ”€â”€ {subdir}/\n")
        f.write("â””â”€â”€ 09_æœªåˆ†ç±»PNG/  # æœªèƒ½è‡ªåŠ¨åˆ†ç±»çš„PNGæ–‡ä»¶\n")
        f.write("```\n\n")

        f.write("## ğŸ’¡ ä½¿ç”¨è¯´æ˜\n\n")
        f.write("1. **æŒ‰åŠŸèƒ½æŸ¥æ‰¾**: æ¯ä¸ªä¸»ç›®å½•å¯¹åº”åº”ç”¨çš„ä¸€ä¸ªåŠŸèƒ½æ¨¡å—\n")
        f.write("2. **æŒ‰ç±»å‹ç»†åˆ†**: å­ç›®å½•è¿›ä¸€æ­¥æŒ‰UIå…ƒç´ ç±»å‹åˆ†ç±»\n")
        f.write("3. **æ–‡æœ¬å…ƒç´ **: æŸ¥çœ‹å„ç›®å½•ä¸‹çš„ `_æ–‡æœ¬å…ƒç´ æ¸…å•.md` äº†è§£å¯å¯¼å‡ºçš„æ–‡æœ¬PNG\n")
        f.write("4. **Sketchå¯¼å‡º**: ç›´æ¥ä»Sketchæ‹–æ‹½å…ƒç´ åˆ°æ¡Œé¢å³å¯ç”ŸæˆPNG\n\n")

        f.write("## ğŸ¯ åˆ†ç±»è§„åˆ™\n\n")
        for category, info in hierarchy_structure.items():
            f.write(f"### {category}\n")
            f.write(f"**æè¿°**: {info['description']}\n\n")
            f.write("**å…³é”®è¯**: " + ", ".join(f"`{kw}`" for kw in info['keywords']) + "\n\n")
            f.write("**å­åˆ†ç±»**:\n")
            for subdir, keywords in info['subdirs'].items():
                f.write(f"- {subdir}: " + ", ".join(f"`{kw}`" for kw in keywords) + "\n")
            f.write("\n")


if __name__ == "__main__":
    main()